{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edf2dff",
   "metadata": {},
   "source": [
    "# Why You (Likely) Do Not Need a Vector Database\n",
    "\n",
    "First, some quick background. The large language models (LLMs) like GPT, Llama, etc all take a limited amount of context (16k tokens). As such if I want to use an LLM to perform question answering on a very large collection of documents, we cannot simply stuff all the documents into the context. While one could fine-tune the LLM on the set of documents, this is difficult and costly if the set of documents change very quickly. For instance, I have fantasized about running Llama on my laptop so I can easily find emails, documents, etc, and I certainly do want to finetune an LLM for every email I receive.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) has risen as a solution where you first try to find a small set of relevant documents that will help answer the question, and stuff just those documents into the context. A popular way to find these set of relevant documents is to compute a vector embedding of each document, which is a representation of the \"meaning\" of the document; documents which are similar semantically, should have vectors which are close. Then given a question, we  compute a vector embedding of the question and use nearest neighbor search to find the most relevant documents.\n",
    "\n",
    "Vector databases have risen in popularity lately as a means of storing and computing nearest neighbor on a large collection of documents. However, I argue that you almost never need a vector database.\n",
    "\n",
    "The task of finding a small set of documents that answers a given question is basically that of Information Retrieval, and very much predates vector databases. The most obvious forms of such systems which you interact with on a daily basis, are search engines (Google, Bing, Apache Lucene, Apple Spotlight, and many others). Of which highly scalable technologies such as reverse indexes are well known, highly available, and have been well developed over decades of research and engineering. Here, I will attempt to demonstrate why you probably do not need a vector database, and maybe some circumstances in which you might.\n",
    "\n",
    "# Dataset\n",
    "First, I need a benchmark dataset for document retrieval for the purposes of question answering. Here, we (mis)use the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "The SQuAD dataset comprises of a collection of paragraphs, and questions for each paragraph. Each question is meant to answered only by information found in the paragraph. \n",
    "For instance a paragraph might be:\n",
    "```\n",
    "Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyoncé\\'s name is a tribute to her mother\\'s maiden name. Beyoncé\\'s younger sister Solange is also a singer and a former member of Destiny\\'s Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.\n",
    "```\n",
    "\n",
    "And a few questions for this paragraph are:\n",
    " - What race was Beyonce's father?\n",
    " - Beyoncé was raised in what religion?\n",
    " - What is the name of Beyoncé's younger sister?\n",
    " \n",
    "However, instead of answering the questions using the provided paragraph, we are going to *invert* the problem: given the question, find the paragraph containing the answer.\n",
    "\n",
    "The dataset is not perfect for this purpose. For instance, there are questions such as \"In what R&B group was she the lead singer?\" which assume that there is sufficient context to disambiguate the pronoun \"she\". However, the *vast* majority of questions do not have this issue, and so suffices for our experiment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eea0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a convenient little jupyter extension to memoize the result of costly / slow computations \n",
    "# (like openai calls etc). This memo is stored together with the repo. and makes things a lot easier to run.\n",
    "# You do not have to selectively pickle and unpickle files and can always just run the notebook straight through.\n",
    "%reload_ext xmemo_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea69652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import openai\n",
    "import functools\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e439c9",
   "metadata": {},
   "source": [
    "# Process SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4290fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json('data/squad2.json')['data']\n",
    "all_paragraphs = [paragraph['context'] for ent in d for paragraph in ent['paragraphs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ace2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. list comprehension is still possible but it gets a little obnoxious.\n",
    "all_qa = []\n",
    "paragraph_id = 0\n",
    "for ent in d:\n",
    "    for paragraph in ent['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0 and qa['is_impossible'] == False:\n",
    "                all_qa.append((paragraph_id, qa['question'], qa['answers'][0]['text']))\n",
    "        paragraph_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a08914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9bf620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19035"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f737ac0",
   "metadata": {},
   "source": [
    "### There are ~ 87k questions and 19k paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e58eb",
   "metadata": {},
   "source": [
    "### Some paragraph samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c75ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyoncé's name is a tribute to her mother's maiden name. Beyoncé's younger sister Solange is also a singer and a former member of Destiny's Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.\n",
      "Beyoncé's first solo recording was a feature on Jay Z's \"'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album's lead single, \"Crazy in Love\", featuring Jay Z, became Beyoncé's first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned Beyoncé a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.\n"
     ]
    }
   ],
   "source": [
    "print(all_paragraphs[3])\n",
    "print(all_paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9fe05",
   "metadata": {},
   "source": [
    "### Some question samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88193838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'Where did Beyonce get her name from?', \"her mother's maiden name\"), (3, \"What race was Beyonce's father?\", 'African-American')]\n",
      "[(10, \"Beyonce's first solo album in the U.S. with what artist in the lead single?\", 'Jay Z'), (10, 'What solo album did Beyonce release in 2003?', 'Dangerously in Love')]\n"
     ]
    }
   ],
   "source": [
    "print(all_qa[40:42])\n",
    "print(all_qa[130:132])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e71a64",
   "metadata": {},
   "source": [
    "# Open AI RAG QA Example\n",
    "Just to give a quick example of how RAG might be used on this dataset, we build a simple question answering agent which answers a given quesion using a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c6be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(max_retries=5,timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733e2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answerer(client, question, context):\n",
    "    system_prompt = \"You are an assistant for question-answering tasks. Use the provided pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Provide just the answer in as few words as possible. Do not use complete sentences.\"\n",
    "    user_prompt = f\"Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59ff44",
   "metadata": {},
   "source": [
    "This is the first question answer pair we have in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7667d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question:  When did Beyonce start becoming popular?\n",
      "Correct Answer:  in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "para_id, question, answer = all_qa[0]\n",
    "print(\"Context: \", all_paragraphs[para_id])\n",
    "print(\"Question: \", question)\n",
    "print(\"Correct Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e158c48",
   "metadata": {},
   "source": [
    "And here is the response from OpenAI. It seems to work pretty well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cfc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late 1990s\n"
     ]
    }
   ],
   "source": [
    "response = question_answerer(client, question, all_paragraphs[para_id])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d34df2",
   "metadata": {},
   "source": [
    "However, how do we tell that \"in the late 1990s\" is the same answer and \"Late 1990s\"? There are some heuristics like BLEU which can be used. However,a lot of careful normalization is needed. For instance how do we deal with different ways of expressing numbers such as \"10\" vs \"ten\"? \"1990s\" vs \"90s\"? \"1,200\" vs \"1200\"? Also the answers in the dataset are  pretty short, so a threshold can be quite difficult to define. \n",
    "\n",
    "So how do we evaluate? Use an LLM to evaluate the answers too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6194d",
   "metadata": {},
   "source": [
    "## Using an LLM to compare answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c170fa8",
   "metadata": {},
   "source": [
    "It has been shown in many domains that an LLMs can actually match human performance in evaluating answers. And the intuition behind this is that it is easier to evaluate than it create. So here we use an LLM to compare two answers to a question. We are simply asking the LLM if the answers mean the same thing. And it seems to work quite nicely for our purposes. Of course, for a real application, we would actually like to evaluate this evaluator against a real dataset, but we are just doing a quick and dirty experiment here. So this will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ca25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same(client, question, a1, a2):\n",
    "    system_prompt = \"You are an assistant for scoring answers. Two answers to a hypothetical question are provided. Say 'Yes' if both answers have the same meaning, and 'No' otherwise.\"\n",
    "    user_prompt = f\"Question: {question} \\Answer 1: {a1} \\nAnswer 2: {a2}\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    response = response.choices[0].message.content == 'Yes'\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec0ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the late 1990s'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc2bbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, question, answer, \"Late 1990s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ae3d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, question, answer, \"Late 1980s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f852e",
   "metadata": {},
   "source": [
    "# Evaluate RAG\n",
    "We can now evalute the performance our RAG set up!. Just to keep things cheap and fast, we just run a 1% subset and take a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c40dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sub_qa = all_qa[0:len(all_qa):100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34544389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff273c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_work(client, question, true_answer, context):\n",
    "    response = question_answerer(client, question, context)\n",
    "    evaluation = is_same(client, question, true_answer, response)\n",
    "    return true_answer, response, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf96f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 77fd21df8c8e7900f005091fa11c661fd69c29ddcda6c2ef56266297123ae245.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=sub_qa output=responses,evaluations\n",
    "responses = []\n",
    "evaluations = []\n",
    "ctr = 0\n",
    "for ent in sub_qa:\n",
    "    para_id, question, true_answer = ent\n",
    "    try:\n",
    "        true_answer, response, evaluation = do_work(client, question, true_answer, all_paragraphs[para_id])\n",
    "    except openai.RateLimitError:\n",
    "        time.sleep(10)\n",
    "        true_answer, response, evaluation = do_work(client, question, true_answer, all_paragraphs[para_id])\n",
    "    ctr += 1\n",
    "    print(f\"Correct Answer: {true_answer}, Response: {response}, Eval: {evaluation}. {ctr}/{len(sub_qa)}\")\n",
    "    responses.append(response)\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08e2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 558ea79e9db37829c5bd21fe76b8a9643ad70d8cf792e1a39df746e271953828.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=sub_qa,responses,evaluations output=df\n",
    "df = pd.DataFrame()\n",
    "df['context'] = [all_paragraphs[i[0]] for i in sub_qa]\n",
    "df['question'] = [i[1] for i in sub_qa]\n",
    "df['answer'] = [i[2] for i in sub_qa]\n",
    "df['response'] = responses\n",
    "df['evaluations'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3c1e4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "      <th>evaluations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>Late 1990s</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The remaining band members recorded \"Independe...</td>\n",
       "      <td>How many weeks did their single \"Independent W...</td>\n",
       "      <td>eleven</td>\n",
       "      <td>11 weeks</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At the 52nd Annual Grammy Awards, Beyoncé rece...</td>\n",
       "      <td>How many awards was Beyonce nominated for at t...</td>\n",
       "      <td>ten</td>\n",
       "      <td>Ten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At the 57th Annual Grammy Awards in February 2...</td>\n",
       "      <td>Which artist beat Beyonce out for Album of the...</td>\n",
       "      <td>Beck</td>\n",
       "      <td>Beck</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forbes magazine began reporting on Beyoncé's e...</td>\n",
       "      <td>In 2012 who placed Beyonce at 16 in the Celebr...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Each year, nearly $200 million in hunters' fed...</td>\n",
       "      <td>What does land has Federal Duck Stamp money he...</td>\n",
       "      <td>5,200,000 acres</td>\n",
       "      <td>National Wildlife Refuge System lands.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>On 16 March 1934, President Franklin D. Roosev...</td>\n",
       "      <td>What act was signed in 1934?</td>\n",
       "      <td>Migratory Bird Hunting Stamp Act</td>\n",
       "      <td>Migratory Bird Hunting Stamp Act</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Kathmandu is located in the northwestern part ...</td>\n",
       "      <td>What river is south of Kathmandu?</td>\n",
       "      <td>Bagmati</td>\n",
       "      <td>Bagmati</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>Swayambhu is a Buddhist stupa atop a hillock a...</td>\n",
       "      <td>In what part of Kathmandu is Swayambhu located?</td>\n",
       "      <td>northwestern</td>\n",
       "      <td>Northwestern part</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>Institute of Medicine, the central college of ...</td>\n",
       "      <td>What institution of tertiary education is know...</td>\n",
       "      <td>National Academy of Medical Sciences</td>\n",
       "      <td>National Academy of Medical Sciences</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context   \n",
       "0    Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \\\n",
       "1    The remaining band members recorded \"Independe...   \n",
       "2    At the 52nd Annual Grammy Awards, Beyoncé rece...   \n",
       "3    At the 57th Annual Grammy Awards in February 2...   \n",
       "4    Forbes magazine began reporting on Beyoncé's e...   \n",
       "..                                                 ...   \n",
       "864  Each year, nearly $200 million in hunters' fed...   \n",
       "865  On 16 March 1934, President Franklin D. Roosev...   \n",
       "866  Kathmandu is located in the northwestern part ...   \n",
       "867  Swayambhu is a Buddhist stupa atop a hillock a...   \n",
       "868  Institute of Medicine, the central college of ...   \n",
       "\n",
       "                                              question   \n",
       "0             When did Beyonce start becoming popular?  \\\n",
       "1    How many weeks did their single \"Independent W...   \n",
       "2    How many awards was Beyonce nominated for at t...   \n",
       "3    Which artist beat Beyonce out for Album of the...   \n",
       "4    In 2012 who placed Beyonce at 16 in the Celebr...   \n",
       "..                                                 ...   \n",
       "864  What does land has Federal Duck Stamp money he...   \n",
       "865                       What act was signed in 1934?   \n",
       "866                  What river is south of Kathmandu?   \n",
       "867    In what part of Kathmandu is Swayambhu located?   \n",
       "868  What institution of tertiary education is know...   \n",
       "\n",
       "                                   answer   \n",
       "0                       in the late 1990s  \\\n",
       "1                                  eleven   \n",
       "2                                     ten   \n",
       "3                                    Beck   \n",
       "4                                  Forbes   \n",
       "..                                    ...   \n",
       "864                       5,200,000 acres   \n",
       "865      Migratory Bird Hunting Stamp Act   \n",
       "866                               Bagmati   \n",
       "867                          northwestern   \n",
       "868  National Academy of Medical Sciences   \n",
       "\n",
       "                                   response  evaluations  \n",
       "0                                Late 1990s         True  \n",
       "1                                  11 weeks         True  \n",
       "2                                       Ten         True  \n",
       "3                                      Beck         True  \n",
       "4                                    Forbes         True  \n",
       "..                                      ...          ...  \n",
       "864  National Wildlife Refuge System lands.         True  \n",
       "865        Migratory Bird Hunting Stamp Act         True  \n",
       "866                                 Bagmati         True  \n",
       "867                       Northwestern part         True  \n",
       "868    National Academy of Medical Sciences         True  \n",
       "\n",
       "[869 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb69d44",
   "metadata": {},
   "source": [
    "# OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c02a61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "def embed_entry(text):\n",
    "    global ctr\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.embeddings.create(input=text, model='text-embedding-ada-002', encoding_format='float')\n",
    "            ctr += 1\n",
    "            if ctr % 100 == 0:\n",
    "                print(ctr)\n",
    "        except openai.RateLimitError as e:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def generate_openai_embeddings(client, textlist):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        ret = list(executor.map(embed_entry, textlist))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71d9ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 6f9fdedb188cbc14a74921efe44e51202f2878fd483cc3648fa869888fd39322.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=embed_entry,all_paragraphs output=par_embeddings\n",
    "par_embeddings = generate_openai_embeddings(client, all_paragraphs)\n",
    "par_embeddings = np.array(par_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f93a90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 877197d865d11dd8a0d7f8f600194670b0ece3e1a13532555d3b012f96525dbb.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=embed_entry,all_qa output=qn_embeddings\n",
    "qn_embeddings = generate_openai_embeddings(client, [q[1] for q in all_qa])\n",
    "qn_embeddings = np.array(qn_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee70c3",
   "metadata": {},
   "source": [
    "# Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df8ae062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4235e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_embeddings_arr = par_embeddings\n",
    "qn_embeddings_arr = qn_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faac1647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 6eacf75df40188544016f9917529add051be3798b90cedd807fa18af519fc760.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=par_embeddings_arr,qn_embeddings_arr output=knn\n",
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree', n_jobs=-1)\n",
    "nbrs.fit(par_embeddings_arr)\n",
    "knn = nbrs.kneighbors(qn_embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7f3081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.53342373, 0.54183945, 0.5480534 , ..., 0.55267372, 0.55477935,\n",
       "         0.55652812],\n",
       "        [0.51626362, 0.56800777, 0.57580097, ..., 0.6029241 , 0.60320468,\n",
       "         0.60361051],\n",
       "        [0.47844263, 0.49059912, 0.51451067, ..., 0.5436377 , 0.54413847,\n",
       "         0.545934  ],\n",
       "        ...,\n",
       "        [0.57681319, 0.59892919, 0.60062395, ..., 0.6253042 , 0.62548659,\n",
       "         0.62892828],\n",
       "        [0.49743956, 0.57099157, 0.57194359, ..., 0.59779   , 0.60029111,\n",
       "         0.60105112],\n",
       "        [0.6205644 , 0.67376102, 0.68988245, ..., 0.70489665, 0.70647834,\n",
       "         0.70827299]]),\n",
       " array([[   23,    45,    26, ...,    50,     0,    14],\n",
       "        [    4,     5,     3, ...,    64,    42,    20],\n",
       "        [    1,    11,     8, ...,     7,    23,    36],\n",
       "        ...,\n",
       "        [18979, 18929, 18925, ..., 18966, 18972, 18934],\n",
       "        [18979, 18932, 18940, ..., 18978, 18972, 18941],\n",
       "        [18979, 18255,  3350, ...,   126, 13786, 11440]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/openai_knn_qn_to_paragraph.npy', knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924ae964",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nearest_paragraphs = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e9c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_paragraphs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa7e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([i[0] for i in all_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f1bcc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at = []\n",
    "p_at.append((nearest_paragraphs[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (nearest_paragraphs[:,i] == truth).sum()\n",
    "    p_at.append(p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e7395aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54566, 63738, 67928, 70421, 72127, 73358, 74400, 75232, 75857, 76395]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4c925",
   "metadata": {},
   "source": [
    "# Simpler Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20857323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45b2f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ylow/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11003763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "import concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9eb5ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    words = nltk.tokenize.word_tokenize(sentence)\n",
    "    return [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a15f1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenize(par) for par in all_paragraphs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cdd157c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bm25_helpers' from '/Users/ylow/xetrepos/RagIRBench/bm25_helpers.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import bm25_helpers\n",
    "importlib.reload(bm25_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8562ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "10852\n",
      "16278\n",
      "21705\n",
      "27131\n",
      "32557\n",
      "37984\n",
      "43410\n",
      "48836\n",
      "54263\n",
      "59689\n",
      "65115\n",
      "70542\n",
      "75968\n",
      "81394\n",
      "86821\n"
     ]
    }
   ],
   "source": [
    "bm25_nearest_paragraphs = []\n",
    "futures = []\n",
    "ctr = 0\n",
    "nworkers = 8\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for s in range(nworkers):\n",
    "        start = (s * len(all_qa)) // nworkers\n",
    "        end = ((s + 1) * len(all_qa)) // nworkers\n",
    "        if s == nworkers - 1:\n",
    "            end = len(all_qa)\n",
    "        futures.append(executor.submit(bm25_helpers.get_bm25_top_10_batch, bm25, [qa[1] for qa in all_qa[start:end]]))\n",
    "\n",
    "    for future in futures:\n",
    "        bm25_nearest_paragraphs.extend(future.result())\n",
    "        print(len(bm25_nearest_paragraphs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b01b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.array(bm25_nearest_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a25a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/bm25_knn_qn_to_paragraph.npy', bm25_nearest_paragraphs_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f61e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.load('data/bm25_knn_qn_to_paragraph.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef825211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6025, 15710, 10808, ..., 11323,  5977, 17616],\n",
       "       [ 9402,  9371, 10274, ..., 11248, 17503,  1679],\n",
       "       [   36,     8,     1, ...,     7,  3685,    21],\n",
       "       ...,\n",
       "       [18938, 18935, 18936, ..., 18945, 18933, 18963],\n",
       "       [18939, 18945, 18940, ..., 18976, 18932, 18934],\n",
       "       [14150, 14946,  1576, ..., 13842,  9371, 10836]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_nearest_paragraphs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baebadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_p_at = []\n",
    "bm25_p_at.append((bm25_nearest_paragraphs_arr[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (bm25_nearest_paragraphs_arr[:,i] == truth).sum()\n",
    "    bm25_p_at.append(bm25_p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7d024d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42140, 49077, 52628, 54824, 56524, 57858, 58863, 59752, 60493, 61126]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_p_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64e499",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb8d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_augment_cache = {}\n",
    "try:\n",
    "    query_augment_cache = pickle.load(open('data/query_augment_cache.pickle','rb'))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17b516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_augment(client, question):\n",
    "    global query_augment_cache\n",
    "    key = question\n",
    "    if key in query_augment_cache:\n",
    "        return query_augment_cache[key]\n",
    "\n",
    "    system_prompt = \"You are to perform query augmentation for a question answering system. For the provided question, return a comma separated list of up to 5 new words which are not part of the question that can be used to improve document retrieval. These words will be added to the words already in the question and will be used to search a collection of documents to find a document which may contain the answer.\"\n",
    "    user_prompt = f\"Question: {question}\"\n",
    "    message = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                }]\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                    messages=message,\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=0.0)\n",
    "        except openai.RateLimitError as e:\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    # split by , \n",
    "    r = response.choices[0].message.content\n",
    "    response = response.choices[0].message.content.split(',')\n",
    "    # split by space\n",
    "    response = [r.strip().split(' ') for r in response]\n",
    "    # flatten\n",
    "    response = [word for i in response for word in i]\n",
    "    response = list(set(response))\n",
    "    query_augment_cache[key] = response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6be50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(max_retries=5,timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17b4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent\n",
    "question_augmentation = []\n",
    "def augment_lambda(ent):\n",
    "    ret = query_augment(client, ent[1])\n",
    "    print(ret)\n",
    "    return ret\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    question_augmentation = list(executor.map(augment_lambda, all_qa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed0829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/query_augment_cache.pickle','wb')\n",
    "pickle.dump(query_augment_cache, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68c8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/qa_question_augmentation.pickle','wb')\n",
    "pickle.dump(question_augmentation, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7a54b",
   "metadata": {},
   "source": [
    "# Using Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49eb17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "tokenized_corpus = [bm25_helpers.tokenize(par) for par in all_paragraphs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a7a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10852\n",
      "21705\n",
      "32557\n",
      "43410\n",
      "54263\n",
      "65115\n",
      "75968\n",
      "86821\n"
     ]
    }
   ],
   "source": [
    "bm25_nearest_paragraphs = []\n",
    "futures = []\n",
    "ctr = 0\n",
    "nworkers = 8\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for s in range(nworkers):\n",
    "        start = (s * len(all_qa)) // nworkers\n",
    "        end = ((s + 1) * len(all_qa)) // nworkers\n",
    "        if s == nworkers - 1:\n",
    "            end = len(all_qa)\n",
    "        futures.append(executor.submit(bm25_helpers.get_bm25_top_10_batch_augmented, bm25, \n",
    "                                       [qa[1] for qa in all_qa[start:end]],\n",
    "                                       question_augmentation[start:end]))\n",
    "\n",
    "    for future in futures:\n",
    "        bm25_nearest_paragraphs.extend(future.result())\n",
    "        print(len(bm25_nearest_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95d74c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.array(bm25_nearest_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a516e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/bm25_knn_qn_to_paragraph_augmented.npy', bm25_nearest_paragraphs_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cf2f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([i[0] for i in all_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3a677d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_p_at = []\n",
    "bm25_p_at.append((bm25_nearest_paragraphs_arr[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (bm25_nearest_paragraphs_arr[:,i] == truth).sum()\n",
    "    bm25_p_at.append(bm25_p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8599c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50821, 59938, 64156, 66789, 68640, 70104, 71252, 72189, 73027, 73692]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_p_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135ed91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
