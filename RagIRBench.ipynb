{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edf2dff",
   "metadata": {},
   "source": [
    "# Why You (Likely) Do Not Need a Vector Database\n",
    "\n",
    "First, some quick background. The large language models (LLMs) like GPT, Llama, etc all take a limited amount of context (16k tokens). As such if I want to use an LLM to perform question answering on a very large collection of documents, we cannot simply stuff all the documents into the context. While one could fine-tune the LLM on the set of documents, this is difficult and costly if the set of documents change very quickly. For instance, I have fantasized about running Llama on my laptop so I can easily find emails, documents, etc, and I certainly do want to finetune an LLM for every email I receive.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) has risen as a solution where you first try to find a small set of relevant documents that will help answer the question, and stuff just those documents into the context. A popular way to find these set of relevant documents is to compute a vector embedding of each document, which is a representation of the \"meaning\" of the document; documents which are similar semantically, should have vectors which are close. Then given a question, we  compute a vector embedding of the question and use nearest neighbor search to find the most relevant documents.\n",
    "\n",
    "Vector databases have risen in popularity lately as a means of storing and computing nearest neighbor on a large collection of documents. However, I argue that you almost never need a vector database.\n",
    "\n",
    "The task of finding a small set of documents that answers a given question is basically that of Information Retrieval, and very much predates vector databases. The most obvious forms of such systems which you interact with on a daily basis, are search engines (Google, Bing, Apache Lucene, Apple Spotlight, and many others). Of which highly scalable technologies such as reverse indexes are well known, highly available, and have been well developed over decades of research and engineering. Here, I will attempt to demonstrate why you probably do not need a vector database, and maybe some circumstances in which you might.\n",
    "\n",
    "# Dataset\n",
    "First, I need a benchmark dataset for document retrieval for the purposes of question answering. Here, we (mis)use the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "The SQuAD dataset comprises of a collection of paragraphs, and questions for each paragraph. Each question is meant to answered only by information found in the paragraph. \n",
    "For instance a paragraph might be:\n",
    "```\n",
    "Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyoncé\\'s name is a tribute to her mother\\'s maiden name. Beyoncé\\'s younger sister Solange is also a singer and a former member of Destiny\\'s Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.\n",
    "```\n",
    "\n",
    "And a few questions for this paragraph are:\n",
    " - What race was Beyonce's father?\n",
    " - Beyoncé was raised in what religion?\n",
    " - What is the name of Beyoncé's younger sister?\n",
    " \n",
    "However, instead of answering the questions using the provided paragraph, we are going to *invert* the problem: given the question, find the paragraph containing the answer.\n",
    "\n",
    "The dataset is not perfect for this purpose. For instance, there are questions such as \"In what R&B group was she the lead singer?\" which assume that there is sufficient context to disambiguate the pronoun \"she\". However, the *vast* majority of questions do not have this issue, and so suffices for our experiment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac6b4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a convenient little jupyter extension to memoize the result of costly / slow computations \n",
    "# (like openai calls etc). This memo is stored together with the repo. and makes things a lot easier to run.\n",
    "# You do not have to selectively pickle and unpickle files and can always just run the notebook straight through.\n",
    "%reload_ext xmemo_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cea69652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import openai\n",
    "import functools\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e439c9",
   "metadata": {},
   "source": [
    "# Process SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4290fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json('data/squad2.json')['data']\n",
    "all_paragraphs = [paragraph['context'] for ent in d for paragraph in ent['paragraphs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ace2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. list comprehension is still possible but it gets a little obnoxious.\n",
    "all_qa = []\n",
    "paragraph_id = 0\n",
    "for ent in d:\n",
    "    for paragraph in ent['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0 and qa['is_impossible'] == False:\n",
    "                all_qa.append((paragraph_id, qa['question'], qa['answers'][0]['text']))\n",
    "        paragraph_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a08914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9bf620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19035"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f737ac0",
   "metadata": {},
   "source": [
    "### There are ~ 87k questions and 19k paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e58eb",
   "metadata": {},
   "source": [
    "### Some paragraph samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c75ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyoncé's name is a tribute to her mother's maiden name. Beyoncé's younger sister Solange is also a singer and a former member of Destiny's Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.\n",
      "Beyoncé's first solo recording was a feature on Jay Z's \"'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album's lead single, \"Crazy in Love\", featuring Jay Z, became Beyoncé's first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned Beyoncé a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.\n"
     ]
    }
   ],
   "source": [
    "print(all_paragraphs[3])\n",
    "print(all_paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9fe05",
   "metadata": {},
   "source": [
    "### Some question samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88193838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'Where did Beyonce get her name from?', \"her mother's maiden name\"), (3, \"What race was Beyonce's father?\", 'African-American')]\n",
      "[(10, \"Beyonce's first solo album in the U.S. with what artist in the lead single?\", 'Jay Z'), (10, 'What solo album did Beyonce release in 2003?', 'Dangerously in Love')]\n"
     ]
    }
   ],
   "source": [
    "print(all_qa[40:42])\n",
    "print(all_qa[130:132])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e71a64",
   "metadata": {},
   "source": [
    "# Open AI RAG QA Example\n",
    "Just to give a quick example of how RAG might be used on this dataset, we build a simple question answering agent which answers a given quesion using a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c6be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(max_retries=5,timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733e2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answerer(client, question, context):\n",
    "    system_prompt = \"You are an assistant for question-answering tasks. Use the provided pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Provide just the answer in as few words as possible. Do not use complete sentences.\"\n",
    "    user_prompt = f\"Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fa259",
   "metadata": {},
   "source": [
    "This is the first question answer pair we have in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7667d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "When did Beyonce start becoming popular?\n",
      "in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "para_id, question, answer = all_qa[0]\n",
    "print(\"Context: \", all_paragraphs[para_id])\n",
    "print(\"Question: \", question)\n",
    "print(\"Correct Answer: \": answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bd9a7",
   "metadata": {},
   "source": [
    "And here is the response from OpenAI. It seems to work pretty well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cfc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late 1990s\n"
     ]
    }
   ],
   "source": [
    "response = question_answerer(client, question, all_paragraphs[para_id])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428154c",
   "metadata": {},
   "source": [
    "However, how do we tell that \"in the late 1990s\" is the same answer and \"Late 1990s\"? There are some heuristics like BLEU which can be used. However,a lot of careful normalization is needed. For instance how do we deal with different ways of expressing numbers such as \"10\" vs \"ten\"? \"1990s\" vs \"90s\"? \"1,200\" vs \"1200\"? Also the answers in the dataset are  pretty short, so a threshold can be quite difficult to define. \n",
    "\n",
    "So how do we evaluate? Use an LLM to evaluate the answers too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6194d",
   "metadata": {},
   "source": [
    "## Using an LLM to compare answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a2215",
   "metadata": {},
   "source": [
    "It has been shown in many domains that an LLMs can actually match human performance in evaluating answers. And the intuition behind this is that it is easier to evaluate than it create. So here we use an LLM to compare two answers to a question. We are simply asking the LLM if the answers mean the same thing. And it seems to work quite nicely for our purposes. Of course, for a real application, we would actually like to evaluate this evaluator against a real dataset, but we are just doing a quick and dirty experiment here. So this will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ca25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same(client, question, a1, a2):\n",
    "    system_prompt = \"You are an assistant for scoring answers. Two answers to a hypothetical question are provided. Say 'Yes' if both answers have the same meaning, and 'No' otherwise.\"\n",
    "    user_prompt = f\"Question: {question} \\Answer 1: {a1} \\nAnswer 2: {a2}\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    response = response.choices[0].message.content == 'Yes'\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec0ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the late 1990s'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc2bbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, question, answer, \"Late 1990s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ae3d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, question, answer, \"Late 1980s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f852e",
   "metadata": {},
   "source": [
    "# Evaluate RAG\n",
    "We can now evalute the performance our RAG set up!. Just to keep things cheap and fast, we just run a 1% subset and take a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c40dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sub_qa = all_qa[0:len(all_qa):100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34544389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff273c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_work(client, question, true_answer, context):\n",
    "    response = question_answerer(client, question, context)\n",
    "    evaluation = is_same(client, question, true_answer, response)\n",
    "    return true_answer, response, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf96f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: in the late 1990s, Response: Late 1990s, Eval: True. 1/869\n",
      "Correct Answer: eleven, Response: 11 weeks, Eval: True. 2/869\n",
      "Correct Answer: ten, Response: Ten, Eval: True. 3/869\n",
      "Correct Answer: Beck, Response: Beck, Eval: True. 4/869\n",
      "Correct Answer: Forbes, Response: Forbes, Eval: True. 5/869\n",
      "Correct Answer: Jarett Wieselman, Response: Jarett Wieselman., Eval: True. 6/869\n",
      "Correct Answer: 8 million, Response: 8 million, Eval: True. 7/869\n",
      "Correct Answer: in Destiny's Child's shows and tours, Response: Destiny's Child's shows and tours., Eval: True. 8/869\n",
      "Correct Answer: Polish, Response: Polish., Eval: True. 9/869\n",
      "Correct Answer: Rondo Op. 1., Response: Rondo Op. 1, Eval: True. 10/869\n",
      "Correct Answer: Polish, Response: Polish, Eval: True. 11/869\n",
      "Correct Answer: Pleyel, Response: Pleyel, Eval: True. 12/869\n",
      "Correct Answer: 1830, Response: 1830, Eval: True. 13/869\n",
      "Correct Answer: Clésinger, Response: Clésinger, Eval: True. 14/869\n",
      "Correct Answer: Karol Szymanowski, Response: Karol Szymanowski, Eval: True. 15/869\n",
      "Correct Answer: disciples, Response: Some disciples., Eval: False. 16/869\n",
      "Correct Answer: Kublai, Response: Kublai Khan, Eval: True. 17/869\n",
      "Correct Answer: Altan Khan, Response: Altan Khan, Eval: True. 18/869\n",
      "Correct Answer: IXI, Response: IXI, Eval: True. 19/869\n",
      "Correct Answer: September 12, 2006, Response: September 12, 2006., Eval: True. 20/869\n",
      "Correct Answer: Peter Oppenheimer, Response: Peter Oppenheimer, Eval: True. 21/869\n",
      "Correct Answer: context-sensitive button mechanic, Response: Context-sensitive button mechanic., Eval: True. 22/869\n",
      "Correct Answer: 2006, Response: 2006, Eval: True. 23/869\n",
      "Correct Answer: 1971, Response: 1971, Eval: True. 24/869\n",
      "Correct Answer: February 2015, Response: February 2015, Eval: True. 25/869\n",
      "Correct Answer: Royal Albert Hall, Response: London, Eval: False. 26/869\n",
      "Correct Answer: droughts, Response: Correlation between droughts and earthquakes., Eval: True. 27/869\n",
      "Correct Answer: five, Response: Five, Eval: True. 28/869\n",
      "Correct Answer: 1.94 million, Response: 1.94 million households, Eval: True. 29/869\n",
      "Correct Answer: civil aviation industry, Response: Civil aviation industry, Eval: True. 30/869\n",
      "Correct Answer: Ningbo, Response: Ningbo, Eval: True. 31/869\n",
      "Correct Answer: 1664, Response: 1664, Eval: True. 32/869\n",
      "Correct Answer: Philadelphia, Response: Philadelphia, Eval: True. 33/869\n",
      "Correct Answer: Battery Park City, Response: Battery Park City, Eval: True. 34/869\n",
      "Correct Answer: 14, Response: 14 miles, Eval: True. 35/869\n",
      "Correct Answer: 660 Madison Avenue, Response: 660 Madison Avenue, Eval: True. 36/869\n",
      "Correct Answer: nine, Response: Nine., Eval: True. 37/869\n",
      "Correct Answer: 35, Response: 27, Eval: False. 38/869\n",
      "Correct Answer: four, Response: Four years, Eval: True. 39/869\n",
      "Correct Answer: United States, Response: United States, Eval: True. 40/869\n",
      "Correct Answer: gender and class, Response: gender and class, Eval: True. 41/869\n",
      "Correct Answer: the neighborhood, Response: safety and comfort, Eval: False. 42/869\n",
      "Correct Answer: trees and plants, Response: Deciduous trees, Eval: False. 43/869\n",
      "Correct Answer: water, earth and stone, Response: Water, earth, stone., Eval: True. 44/869\n",
      "Correct Answer: 10, Response: 10, Eval: True. 45/869\n",
      "Correct Answer: Roland TR-808 drum machine, Response: Roland TR-808 drum machine, Eval: True. 46/869\n",
      "Correct Answer: DW Kanye West, Response: DW Kanye West, Eval: True. 47/869\n",
      "Correct Answer: Change.org, Response: Change.org, Eval: True. 48/869\n",
      "Correct Answer: teachings attributed to Gautama Buddha, Response: Teachings attributed to Gautama Buddha., Eval: True. 49/869\n",
      "Correct Answer: the Middle Way, Response: Middle Way, Eval: False. 50/869\n",
      "Correct Answer: ignorance, Response: ignorance, Eval: True. 51/869\n",
      "Correct Answer: a buddha, Response: Buddha., Eval: True. 52/869\n",
      "Correct Answer: mind, Response: mind, Eval: True. 53/869\n",
      "Correct Answer: Prajñāpāramitā, Response: Prajñāpāramitā series, Eval: True. 54/869\n",
      "Correct Answer: Hinayana, Response: Hinayāna, Eval: True. 55/869\n",
      "Correct Answer: producers, Response: Producers, Eval: True. 56/869\n",
      "Correct Answer: A Moment Like This, Response: \"A Moment Like This\", Eval: True. 57/869\n",
      "Correct Answer: Sanjaya Malakar, Response: Sanjaya Malakar, Eval: True. 58/869\n",
      "Correct Answer: Kris Allen, Response: Kris Allen, Eval: True. 59/869\n",
      "Correct Answer: Home, Response: \"Home\", Eval: True. 60/869\n",
      "Correct Answer: guitar, Response: guitar, Eval: True. 61/869\n",
      "Correct Answer: Ken Tucker, Response: Ken Tucker, Eval: True. 62/869\n",
      "Correct Answer: domestic dog, Response: Domestic dog., Eval: True. 63/869\n",
      "Correct Answer: 14 to 15 years., Response: 14 to 15 years, Eval: True. 64/869\n",
      "Correct Answer: four, Response: Four, Eval: True. 65/869\n",
      "Correct Answer: 26, Response: 26, Eval: True. 66/869\n",
      "Correct Answer: Olympia, Greece, Response: Olympia, Greece, Eval: True. 67/869\n",
      "Correct Answer: April 6, Response: April 6, Eval: True. 68/869\n",
      "Correct Answer: Jorge Carcavallo., Response: Jorge Carcavallo, Eval: True. 69/869\n",
      "Correct Answer: 16, Response: 16 km, Eval: True. 70/869\n",
      "Correct Answer: Leong Hong Man, Response: Leong Hong Man, Eval: True. 71/869\n",
      "Correct Answer: mitochondria and chloroplasts, Response: Mitochondria, chloroplasts, Eval: True. 72/869\n",
      "Correct Answer: Helmut Fend, Response: Helmut Fend, Eval: True. 73/869\n",
      "Correct Answer: January 2002, Response: January 2002, Eval: True. 74/869\n",
      "Correct Answer: Sir Robert Walpole, Response: Sir Robert Walpole, Eval: True. 75/869\n",
      "Correct Answer: College of Advanced Education system, Response: College of Advanced Education system, Eval: True. 76/869\n",
      "Correct Answer: three, Response: 3 years, Eval: True. 77/869\n",
      "Correct Answer: Church of Scientology, Response: Church of Scientology, Eval: True. 78/869\n",
      "Correct Answer: lice, Response: Lice, Eval: True. 79/869\n",
      "Correct Answer: the Normandy Landings, Response: Battle of Vimy Ridge, Eval: False. 80/869\n",
      "Correct Answer: CFS Leitrim in Ottawa, Response: CFS Leitrim in Ottawa., Eval: True. 81/869\n",
      "Correct Answer: 1962, Response: 1962, Eval: True. 82/869\n",
      "Correct Answer: 875 CE, Response: 875 CE, Eval: True. 83/869\n",
      "Correct Answer: Specification of illumination requirements, Response: Specification of illumination requirements, Eval: True. 84/869\n",
      "Correct Answer: president, Response: The president., Eval: True. 85/869\n",
      "Correct Answer: architecture, Response: Architecture, Eval: True. 86/869\n",
      "Correct Answer: the school of metaphoric architecture, Response: Metaphoric architecture., Eval: False. 87/869\n",
      "Correct Answer: political, economic, and cultural attributes, Response: geography, climate, flora, Eval: False. 88/869\n",
      "Correct Answer: 16 Portland Place, London, Response: 16 Portland Place, Eval: False. 89/869\n",
      "Correct Answer: seven, Response: Seven, Eval: True. 90/869\n",
      "Correct Answer: 2004, Response: 2004, Eval: True. 91/869\n",
      "Correct Answer: 5'10\", Response: 5'10\", Eval: True. 92/869\n",
      "Correct Answer: 1086, Response: 1086, Eval: True. 93/869\n",
      "Correct Answer: University of Southampton, Response: University of Southampton, Eval: True. 94/869\n",
      "Correct Answer: ITV West Country, Response: ITV West Country, Eval: True. 95/869\n",
      "Correct Answer: orthodox, Response: Heresy, Eval: False. 96/869\n",
      "Correct Answer: Hastings Ismay, Response: Hastings Ismay, Eval: True. 97/869\n",
      "Correct Answer: Herod Agrippa II, Response: Herod Agrippa II, Eval: True. 98/869\n",
      "Correct Answer: 1968, Response: 1968, Eval: True. 99/869\n",
      "Correct Answer: 230,233 households, Response: 230,233, Eval: True. 100/869\n",
      "Correct Answer: The Campus, Response: The Campus, Eval: True. 101/869\n",
      "Correct Answer: matrilocal, Response: Flexible kinship, Eval: False. 102/869\n",
      "Correct Answer: Dr Nafis Sadik, Response: Dr Nafis Sadik, Eval: True. 103/869\n",
      "Correct Answer: December 25, 1991, Response: December 25, 1991, Eval: True. 104/869\n",
      "Correct Answer: the southernmost Kurils, Response: Kuril Islands, Eval: True. 105/869\n",
      "Correct Answer: Professor Henry Higgins, Response: Professor Henry Higgins, Eval: True. 106/869\n",
      "Correct Answer: Civil War, Response: Civil War, Eval: True. 107/869\n",
      "Correct Answer: France, Response: France, Eval: True. 108/869\n",
      "Correct Answer: 1514, Response: 1514, Eval: True. 109/869\n",
      "Correct Answer: Wetherspoon, Response: Wetherspoon pub chain, Eval: True. 110/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: the Royal George, Response: Royal George, Eval: False. 111/869\n",
      "Correct Answer: The Angel, Response: The Angel, Eval: True. 112/869\n",
      "Correct Answer: multiple ISPs interconnect at peering points or Internet exchange points, Response: Interconnection between ISPs., Eval: False. 113/869\n",
      "Correct Answer: Manga! Manga! The World of Japanese Comics, Response: Manga! Manga! The World of Japanese Comics, Eval: True. 114/869\n",
      "Correct Answer: Royal, Response: Royal Navy, Eval: True. 115/869\n",
      "Correct Answer: emigration, Response: Emigration, Eval: True. 116/869\n",
      "Correct Answer: Christmas Day 1967, Response: Christmas Day 1967, Eval: True. 117/869\n",
      "Correct Answer: Preaspirated stops, Response: preaspirated stops, Eval: True. 118/869\n",
      "Correct Answer: link two metal centers, Response: Metal centers, Eval: False. 119/869\n",
      "Correct Answer: as a tracer gas for minute leak detection, Response: Leak detection., Eval: False. 120/869\n",
      "Correct Answer: Project Mercury, Response: Project Mercury, Eval: True. 121/869\n",
      "Correct Answer: 1958, Response: 1958, Eval: True. 122/869\n",
      "Correct Answer: Chen Fangyun and his colleagues, Response: Chen Fangyun., Eval: False. 123/869\n",
      "Correct Answer: by 2020, Response: 2020, Eval: True. 124/869\n",
      "Correct Answer: their own private systems of canon law, Response: Private systems of canon law., Eval: True. 125/869\n",
      "Correct Answer: 200, Response: 200, Eval: True. 126/869\n",
      "Correct Answer: movable type, Response: Tirant lo Blanc., Eval: False. 127/869\n",
      "Correct Answer: section pronunciation, Response: This article., Eval: False. 128/869\n",
      "Correct Answer: between 2001 and 2004, Response: Between 2001 and 2004., Eval: True. 129/869\n",
      "Correct Answer: The Embargo Act of 1807, Response: Embargo Act of 1807, Eval: True. 130/869\n",
      "Correct Answer: 22, Response: 22, Eval: True. 131/869\n",
      "Correct Answer: three, Response: Three., Eval: True. 132/869\n",
      "Correct Answer: I-90, Response: I-90, Eval: True. 133/869\n",
      "Correct Answer: Margaret Sullavan, Response: Margaret Sullavan, Eval: True. 134/869\n",
      "Correct Answer: Donna Langley, Response: Donna Langley, Eval: True. 135/869\n",
      "Correct Answer: Newer Orthography, Response: Finnish orthography., Eval: False. 136/869\n",
      "Correct Answer: thousandths of an inch, Response: thousandths of an inch, Eval: True. 137/869\n",
      "Correct Answer: 1977, Response: 1977, Eval: True. 138/869\n",
      "Correct Answer: oldies, Response: Soft AC, Eval: False. 139/869\n",
      "Correct Answer: hora tertia, Response: Hora tertia, Eval: True. 140/869\n",
      "Correct Answer: incandescent, Response: Incandescent lighting, Eval: True. 141/869\n",
      "Correct Answer: one, Response: One end rule., Eval: True. 142/869\n",
      "Correct Answer: a dozen, Response: dozen, Eval: False. 143/869\n",
      "Correct Answer: naturalization records, Response: Naturalization records., Eval: True. 144/869\n",
      "Correct Answer: March 2011, Response: 16 March 2011, Eval: True. 145/869\n",
      "Correct Answer: fixed tuition rate, Response: Four-Year Tuition Compact, Eval: False. 146/869\n",
      "Correct Answer: more than 1.4 million residents, Response: 1.4 million residents., Eval: False. 147/869\n",
      "Correct Answer: evergreens and oaks, Response: Evergreens and oaks., Eval: True. 148/869\n",
      "Correct Answer: Kunqu, Response: Kunqu, Eval: True. 149/869\n",
      "Correct Answer: Chicago, Illinois, Response: Chicago, Illinois, Eval: True. 150/869\n",
      "Correct Answer: 50.1, Response: 50.1%, Eval: True. 151/869\n",
      "Correct Answer: nine, Response: 9, Eval: True. 152/869\n",
      "Correct Answer: armed conflict, Response: Armed conflict., Eval: True. 153/869\n",
      "Correct Answer: Italian and Spanish, Response: Italian and Spanish, Eval: True. 154/869\n",
      "Correct Answer: 24.5%, Response: 24.5%, Eval: True. 155/869\n",
      "Correct Answer: government, Response: government, Eval: True. 156/869\n",
      "Correct Answer: 1179, Response: 1179, Eval: True. 157/869\n",
      "Correct Answer: government action or inaction, Response: Government action or inaction., Eval: True. 158/869\n",
      "Correct Answer: kleptocracy, Response: Kleptocracy., Eval: True. 159/869\n",
      "Correct Answer: Woodwind, Response: Woodwind instruments, Eval: True. 160/869\n",
      "Correct Answer: early Christian liturgical music,, Response: Early Christian liturgical music., Eval: True. 161/869\n",
      "Correct Answer: Warner Bros, Response: Warner Bros., Eval: True. 162/869\n",
      "Correct Answer: the Sarmatian, Hun and Gothic empires, Response: Sarmatian, Hun, Gothic empires, Eval: True. 163/869\n",
      "Correct Answer: 2007, Response: 2007, Eval: True. 164/869\n",
      "Correct Answer: Castle Way, Response: Castle Way, Eval: True. 165/869\n",
      "Correct Answer: Hampton, Virginia, Response: Hampton, Virginia., Eval: True. 166/869\n",
      "Correct Answer: the service sector, Response: Service sector, Eval: True. 167/869\n",
      "Correct Answer: 1952, Response: 1952, Eval: True. 168/869\n",
      "Correct Answer: in good or proper form., Response: Communicated and found in good form., Eval: False. 169/869\n",
      "Correct Answer: terminology, Response: Terminology, Eval: True. 170/869\n",
      "Correct Answer: the pre-Constitutional United States Federal government, Response: Pre-Constitutional United States Federal government., Eval: True. 171/869\n",
      "Correct Answer: Moscow, Response: Moscow., Eval: True. 172/869\n",
      "Correct Answer: Nehru, Response: Sukarno, Eval: False. 173/869\n",
      "Correct Answer: German, Response: German, Eval: True. 174/869\n",
      "Correct Answer: 1520s, Response: 1520s, Eval: True. 175/869\n",
      "Correct Answer: 1884, Response: 1884, Eval: True. 176/869\n",
      "Correct Answer: October 2011, Response: October 2011, Eval: True. 177/869\n",
      "Correct Answer: included those almost rich and powerful enough to be magnates down to rascals, Response: Economic status and misconception., Eval: False. 178/869\n",
      "Correct Answer: the extinction of the male-line descendants of the old royal dynasty, Response: Extinction of male-line descendants., Eval: True. 179/869\n",
      "Correct Answer: enormous influence, Response: Enormous influence., Eval: True. 180/869\n",
      "Correct Answer: September 21, 19 BC, Response: September 21, 19 BC., Eval: True. 181/869\n",
      "Correct Answer: As the rising peaks underwent erosion, Response: During the orogeny., Eval: False. 182/869\n",
      "Correct Answer: from 800 to 1,700 m, Response: 800 to 1,700 m (2,625 to 5,577 ft), Eval: True. 183/869\n",
      "Correct Answer: Conrad Gessner, Response: Conrad Gessner, Eval: True. 184/869\n",
      "Correct Answer: hypothetical particles that would mix during reproduction, Response: Hypothetical particles., Eval: False. 185/869\n",
      "Correct Answer: their specific DNA loci, Response: Functional products, Eval: False. 186/869\n",
      "Correct Answer: stabilizing, Response: Stabilizing selection, Eval: True. 187/869\n",
      "Correct Answer: Cuba, Response: Cuba, Eval: True. 188/869\n",
      "Correct Answer: a pact of stability, Response: Pact of stability., Eval: True. 189/869\n",
      "Correct Answer: Hudson River, Response: Hudson River., Eval: True. 190/869\n",
      "Correct Answer: Bellevue, Response: Bellevue Hospital Center, Eval: True. 191/869\n",
      "Correct Answer: 181st Street, Response: 181st Street, Eval: True. 192/869\n",
      "Correct Answer: fluid-filled ventricle, Response: fluid-filled ventricle, Eval: True. 193/869\n",
      "Correct Answer: the middle of the 20th century,, Response: 20th century, Eval: False. 194/869\n",
      "Correct Answer: the Ottoman Empire, Response: Ottoman Empire, Eval: True. 195/869\n",
      "Correct Answer: (troops of British India, Response: Troops of British India, Eval: True. 196/869\n",
      "Correct Answer: Qiantang, Response: Qiantang, Eval: True. 197/869\n",
      "Correct Answer: 400,000, Response: 400,000, Eval: True. 198/869\n",
      "Correct Answer: £178 billion, Response: £178 billion, Eval: True. 199/869\n",
      "Correct Answer: the Soviet Union, Response: Soviet Union, Eval: True. 200/869\n",
      "Correct Answer: ambiguity, Response: Ambiguity., Eval: True. 201/869\n",
      "Correct Answer: 90, Response: 90 degrees, Eval: True. 202/869\n",
      "Correct Answer: modulus of rupture, Response: Modulus of rupture., Eval: True. 203/869\n",
      "Correct Answer: hydrogen, Response: Hydrogen, Eval: True. 204/869\n",
      "Correct Answer: Irir Samaale, Response: Irir Samaale., Eval: True. 205/869\n",
      "Correct Answer: Asia, Response: Asia, Eval: True. 206/869\n",
      "Correct Answer: Sudan, Response: Sudan, Eval: True. 207/869\n",
      "Correct Answer: 1980, Response: 1980, Eval: True. 208/869\n",
      "Correct Answer: 634–635, Response: 634-635, Eval: True. 209/869\n",
      "Correct Answer: 35, Response: 80 million, Eval: False. 210/869\n",
      "Correct Answer: lateen, Response: Lateen sails, Eval: True. 211/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: handbook for witch-hunters, Response: A handbook for witch-hunters., Eval: True. 212/869\n",
      "Correct Answer: 1991, Response: 1991, Eval: True. 213/869\n",
      "Correct Answer: 1927, Response: 1927, Eval: True. 214/869\n",
      "Correct Answer: a control system or central controller), Response: Control system, central controller, Eval: True. 215/869\n",
      "Correct Answer: umm walad, Response: Umm walad, Eval: True. 216/869\n",
      "Correct Answer: Etymologies, Response: Etymologies, Eval: True. 217/869\n",
      "Correct Answer: Clinton used his knowledge of black culture to exploit black people for political gain, Response: Because they believed Clinton exploited black culture for political gain and did not serve black interests., Eval: True. 218/869\n",
      "Correct Answer: 19th century, Response: 19th century, Eval: True. 219/869\n",
      "Correct Answer: 87%, Response: 87%, Eval: True. 220/869\n",
      "Correct Answer: 1931, Response: 1931, Eval: True. 221/869\n",
      "Correct Answer: 35, Response: 35, Eval: True. 222/869\n",
      "Correct Answer: 2011, Response: 2011, Eval: True. 223/869\n",
      "Correct Answer: Australia, Response: Australia, Eval: True. 224/869\n",
      "Correct Answer: one that is entirely innate and another that relies on experience, Response: Radical pair mechanism and magnetites., Eval: False. 225/869\n",
      "Correct Answer: Daniel White, Response: Daniel White, Eval: True. 226/869\n",
      "Correct Answer: 39,558, Response: 39,558, Eval: True. 227/869\n",
      "Correct Answer: typically cellular in nature, Response: Cellular and distributed., Eval: False. 228/869\n",
      "Correct Answer: The psychoacoustic masking codec, Response: Psychoacoustic masking codec., Eval: True. 229/869\n",
      "Correct Answer: MPEG-1, Response: MPEG-1, Eval: True. 230/869\n",
      "Correct Answer: separate from the actual MP3 frame data, Response: Separate from MP3 frame data., Eval: True. 231/869\n",
      "Correct Answer: Jesse Saunders, Response: Jesse Saunders, Eval: True. 232/869\n",
      "Correct Answer: Paradise Garage, Response: Paradise Garage, Eval: True. 233/869\n",
      "Correct Answer: 8, Response: 8, Eval: True. 234/869\n",
      "Correct Answer: 396 billion pesos, Response: 31.1 billion dollars, Eval: False. 235/869\n",
      "Correct Answer: Battle of Celaya, Response: Battle of Celaya, Eval: True. 236/869\n",
      "Correct Answer: working class, Response: working class, Eval: True. 237/869\n",
      "Correct Answer: sound, Response: Sound frequency., Eval: True. 238/869\n",
      "Correct Answer: Petrochemical, Response: Petrochemical refineries., Eval: True. 239/869\n",
      "Correct Answer: Hurricane Rita, Response: Hurricane Rita, Eval: True. 240/869\n",
      "Correct Answer: one of the top two, Response: Top two., Eval: True. 241/869\n",
      "Correct Answer: ships, Response: Aquaculture, Eval: False. 242/869\n",
      "Correct Answer: low-denomination coins, Response: Low-denomination coins, parts exposed to seawater, decorations, lead-free solders., Eval: False. 243/869\n",
      "Correct Answer: cognitive psychology, Response: Cognitive psychology., Eval: True. 244/869\n",
      "Correct Answer: Radio and TV, Response: Media, Eval: False. 245/869\n",
      "Correct Answer: vasodilation, Response: Reduced platelet aggregation and vasodilation., Eval: True. 246/869\n",
      "Correct Answer: via removable media, Response: Removable media., Eval: True. 247/869\n",
      "Correct Answer: rejection of Judaism, Response: Rejected, Eval: False. 248/869\n",
      "Correct Answer: Gromia sphaerica, Response: Giant single-celled protist Gromia sphaerica., Eval: True. 249/869\n",
      "Correct Answer: head retention, Response: Head retention, Eval: True. 250/869\n",
      "Correct Answer: UK, Response: UK, Eval: True. 251/869\n",
      "Correct Answer: E Pluribus Unum, Response: On the edge of the coin., Eval: False. 252/869\n",
      "Correct Answer: Congressional standard of the silver dollar, Response: Silver dollar, Eval: False. 253/869\n",
      "Correct Answer: credit, Response: For credit, Eval: True. 254/869\n",
      "Correct Answer: Herschel Grynszpan, Response: Herschel Grynszpan, Eval: True. 255/869\n",
      "Correct Answer: Feelings, Response: Feelings, Eval: True. 256/869\n",
      "Correct Answer: 2004, Response: 2004, Eval: True. 257/869\n",
      "Correct Answer: David Moyes, Response: David Moyes, Eval: True. 258/869\n",
      "Correct Answer: mōna, Response: mōna, Eval: True. 259/869\n",
      "Correct Answer: Vladivostok, Response: Vladivostok, Eval: True. 260/869\n",
      "Correct Answer: Electromagnetic Aircraft Launch System (EMALS), Response: Electromagnetic Aircraft Launch System (EMALS), Eval: True. 261/869\n",
      "Correct Answer: 2014, Response: 2014, Eval: True. 262/869\n",
      "Correct Answer: 1932, Response: 1932, Eval: True. 263/869\n",
      "Correct Answer: Skype, Response: Skype, Eval: True. 264/869\n",
      "Correct Answer: delay in communication, Response: Delay in communication., Eval: True. 265/869\n",
      "Correct Answer: Sega of America, Response: Sega of America, Eval: True. 266/869\n",
      "Correct Answer: scaling down and in some cases discontinuing sales, Response: Discontinued sales., Eval: False. 267/869\n",
      "Correct Answer: the People's Republic of China, Response: China, Eval: True. 268/869\n",
      "Correct Answer: population expansion, railroad construction, and the disappearance of the buffalo herds, Response: Population expansion, railroad construction, disappearance of buffalo herds., Eval: True. 269/869\n",
      "Correct Answer: Stone Temple Pilots, Response: Stone Temple Pilots, Eval: True. 270/869\n",
      "Correct Answer: up to 160 acres, Response: 160 acres, Eval: True. 271/869\n",
      "Correct Answer: exponential and logistic models, Response: Exponential and logistic models., Eval: True. 272/869\n",
      "Correct Answer: 2050, Response: 2050, Eval: True. 273/869\n",
      "Correct Answer: an application to abort its current operation or to exit (terminate) altogether, Response: Abort or exit, Eval: True. 274/869\n",
      "Correct Answer: an \"overhead charge\", Response: Overhead charge., Eval: True. 275/869\n",
      "Correct Answer: individuals, pairs, trios or groups up to 6 people, Response: 6, Eval: False. 276/869\n",
      "Correct Answer: 2015, Response: 2015, Eval: True. 277/869\n",
      "Correct Answer: Four, Response: Four, Eval: True. 278/869\n",
      "Correct Answer: sixth national priority, Response: Sixth, Eval: True. 279/869\n",
      "Correct Answer: conflict of interest edits, Response: Conflict of interest edits., Eval: True. 280/869\n",
      "Correct Answer: 2005, Response: 2005, Eval: True. 281/869\n",
      "Correct Answer: the Immaculate Conception, Response: Immaculate Conception, Eval: True. 282/869\n",
      "Correct Answer: The British Colonial Office, Response: The British Colonial Office., Eval: True. 283/869\n",
      "Correct Answer: Melbourne, Response: Melbourne, Eval: True. 284/869\n",
      "Correct Answer: Southbank, Response: Southbank, Eval: True. 285/869\n",
      "Correct Answer: 6 April 1199, Response: 6 April 1199, Eval: True. 286/869\n",
      "Correct Answer: Normandy, Response: Normandy, Eval: True. 287/869\n",
      "Correct Answer: his father, Response: his father, Eval: True. 288/869\n",
      "Correct Answer: Pepsi, Response: Apple, Eval: False. 289/869\n",
      "Correct Answer: second, Response: The second generation., Eval: True. 290/869\n",
      "Correct Answer: during the First World War, Response: First World War, Eval: False. 291/869\n",
      "Correct Answer: John M. Browning, Response: John M. Browning, Eval: True. 292/869\n",
      "Correct Answer: small errors in distance, Response: Small errors in distance., Eval: True. 293/869\n",
      "Correct Answer: 1943, Response: 1943, Eval: True. 294/869\n",
      "Correct Answer: 5 to 8, Response: Grades 5 to 8 (Classes V to VIII), Eval: True. 295/869\n",
      "Correct Answer: Silk Exchange, Response: Silk Exchange (Llotja de la Seda) building, Eval: True. 296/869\n",
      "Correct Answer: Moncey, Response: Marshal Moncey, Eval: True. 297/869\n",
      "Correct Answer: Jaime Milans del Bosch, Response: Jaime Milans del Bosch, Eval: True. 298/869\n",
      "Correct Answer: Prussian tactics and organizational skills, Response: Prussian tactics and organizational skills., Eval: True. 299/869\n",
      "Correct Answer: Regular Army, Response: Regular Army, Eval: True. 300/869\n",
      "Correct Answer: a French artillery battery, Response: French artillery battery, Eval: True. 301/869\n",
      "Correct Answer: SMS Arminius, Response: SMS Arminius, Eval: True. 302/869\n",
      "Correct Answer: 30% more, Response: 30%, Eval: False. 303/869\n",
      "Correct Answer: empathy, sharing, and leadership, Response: Empathy, sharing, leadership., Eval: True. 304/869\n",
      "Correct Answer: marijuana, Response: Marijuana, Eval: True. 305/869\n",
      "Correct Answer: cognitive, Response: Cognitive development, Eval: True. 306/869\n",
      "Correct Answer: 1959, Response: 1959, Eval: True. 307/869\n",
      "Correct Answer: 1998, Response: 1998, Eval: True. 308/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: police and public works departments, Response: police and public works departments, Eval: True. 309/869\n",
      "Correct Answer: take action on Ethiopia, Response: Take action on Ethiopia., Eval: True. 310/869\n",
      "Correct Answer: antimony, Response: Antimony, Eval: True. 311/869\n",
      "Correct Answer: hexavalent, Response: hexavalent uranium compounds, Eval: True. 312/869\n",
      "Correct Answer: 845, Response: 845, Eval: True. 313/869\n",
      "Correct Answer: glucose metabolism, Response: Altered glucose metabolism., Eval: True. 314/869\n",
      "Correct Answer: governess, Response: Governess, Eval: True. 315/869\n",
      "Correct Answer: Windlesham Moor, Response: Windlesham Moor, Eval: True. 316/869\n",
      "Correct Answer: ambiguity regarding the definition of sexual orientation, Response: Ambiguity, difficulty determining., Eval: False. 317/869\n",
      "Correct Answer: both the masculine and the feminine sides of their natures, Response: Desires derived from both masculine and feminine sides., Eval: True. 318/869\n",
      "Correct Answer: Recognizing that a large portion of population is not completely heterosexual or homosexual and people can experience both heterosexual and homosexual behavior, Response: To combat the assumption of binary sexual orientations., Eval: False. 319/869\n",
      "Correct Answer: 1994, Response: 1994, Eval: True. 320/869\n",
      "Correct Answer: 1,200, Response: 1,200, Eval: True. 321/869\n",
      "Correct Answer: Minnesota, Response: Minnesota, Eval: True. 322/869\n",
      "Correct Answer: Utah, Response: Utah, Eval: True. 323/869\n",
      "Correct Answer: specific discourse of the genre, Response: specific discourse of the genre, Eval: True. 324/869\n",
      "Correct Answer: The Age of Reason, Response: The Age of Reason, Eval: True. 325/869\n",
      "Correct Answer: 1789, Response: 1789, Eval: True. 326/869\n",
      "Correct Answer: Sega's Master System, Response: Sega's Master System, Eval: True. 327/869\n",
      "Correct Answer: Masayuki Uemura, Response: Masayuki Uemura, Eval: True. 328/869\n",
      "Correct Answer: Letter to the Monks, Response: Apology to Constantius, Eval: False. 329/869\n",
      "Correct Answer: Father of Orthodoxy, Response: Father of Orthodoxy, Eval: True. 330/869\n",
      "Correct Answer: Skid Road, Response: Skid Road, Eval: True. 331/869\n",
      "Correct Answer: Guangdong province, Response: Guangdong province., Eval: True. 332/869\n",
      "Correct Answer: NPR affiliates, Response: KUOW and KPLU are radio stations., Eval: False. 333/869\n",
      "Correct Answer: Working memory, Response: Working memory, Eval: True. 334/869\n",
      "Correct Answer: black (historically) or African American, Response: Black or African American., Eval: True. 335/869\n",
      "Correct Answer: dark skin and ample bodies, Response: Dark skin and ample bodies., Eval: True. 336/869\n",
      "Correct Answer: Stanley Crouch, Response: Stanley Crouch, Eval: True. 337/869\n",
      "Correct Answer: nearly 80%, Response: 80%, Eval: True. 338/869\n",
      "Correct Answer: the Middle East, Response: Middle East, Eval: True. 339/869\n",
      "Correct Answer: Maurice Hilleman, Response: Maurice Hilleman, Eval: True. 340/869\n",
      "Correct Answer: combination therapy, Response: Combination therapy, Eval: True. 341/869\n",
      "Correct Answer: increasingly outsource risks related to fundamental research, Response: Outsourcing risks., Eval: False. 342/869\n",
      "Correct Answer: Maysum, Response: Maysum, Eval: True. 343/869\n",
      "Correct Answer: Abu Muslim, Response: Abu Muslim, Eval: True. 344/869\n",
      "Correct Answer: Ritchie Mines, Response: Ritchie Mines, Eval: True. 345/869\n",
      "Correct Answer: 1 million barrels, Response: 1 million barrels, Eval: True. 346/869\n",
      "Correct Answer: typhoid fever, Response: typhoid fever, Eval: True. 347/869\n",
      "Correct Answer: Melbourne, Response: Melbourne, Eval: True. 348/869\n",
      "Correct Answer: 1972, Response: 1972, Eval: True. 349/869\n",
      "Correct Answer: a haemophiliac, Response: Unknown, Eval: False. 350/869\n",
      "Correct Answer: Germany, Response: Germany, Eval: True. 351/869\n",
      "Correct Answer: 1840, Response: 1840, Eval: True. 352/869\n",
      "Correct Answer: a list, Response: List of recognized Grand Lodges., Eval: False. 353/869\n",
      "Correct Answer: 1933, Response: 1933, Eval: True. 354/869\n",
      "Correct Answer: Pennsylvania, Response: Pennsylvania, Eval: True. 355/869\n",
      "Correct Answer: 1165, Response: 1165, Eval: True. 356/869\n",
      "Correct Answer: 60,000, Response: 60,000, Eval: True. 357/869\n",
      "Correct Answer: US$118 billion, Response: US$118 billion, Eval: True. 358/869\n",
      "Correct Answer: Philip II, Response: Philip II, Eval: True. 359/869\n",
      "Correct Answer: military and paramilitary forces, Response: Military and paramilitary forces., Eval: True. 360/869\n",
      "Correct Answer: Antigonus II Mattathias, Response: Antigonus II Mattathias, Eval: True. 361/869\n",
      "Correct Answer: 2.5, Response: 2.5%, Eval: True. 362/869\n",
      "Correct Answer: C. Préaux, Response: C. Préaux, Eval: True. 363/869\n",
      "Correct Answer: 1814, Response: 1814, Eval: True. 364/869\n",
      "Correct Answer: Uruguay, Response: Uruguay, Eval: True. 365/869\n",
      "Correct Answer: professor Juan Pedro Toni, Response: Juan Pedro Toni, Eval: False. 366/869\n",
      "Correct Answer: By 1940, the quail egg industry was flourishing,, Response: Early 20th century., Eval: False. 367/869\n",
      "Correct Answer: Rhine–Meuse–Scheldt delta, Response: Rhine-Meuse-Scheldt delta, Eval: True. 368/869\n",
      "Correct Answer: 20, Response: 20 universities., Eval: True. 369/869\n",
      "Correct Answer: hen, Response: hen, Eval: True. 370/869\n",
      "Correct Answer: The large East Front, Response: The large East Front., Eval: True. 371/869\n",
      "Correct Answer: 1854, Response: 1854, Eval: True. 372/869\n",
      "Correct Answer: Mercantile Safe Deposit Company, Response: Mercantile Safe Deposit Company in New York City., Eval: False. 373/869\n",
      "Correct Answer: more expensive, so its use is limited to smaller lamps, Response: Expensive and limited use., Eval: True. 374/869\n",
      "Correct Answer: foreign players, Response: foreign players, Eval: True. 375/869\n",
      "Correct Answer: 62,217, Response: 62,217 shares., Eval: True. 376/869\n",
      "Correct Answer: sportswear, Response: Gymnastics garment, Eval: False. 377/869\n",
      "Correct Answer: Comiskey Park, Response: Comiskey Park, Eval: True. 378/869\n",
      "Correct Answer: the second HoHoKam Park, Response: I don't know., Eval: False. 379/869\n",
      "Correct Answer: Kim Il-sung, Response: Kim Il-sung, Eval: True. 380/869\n",
      "Correct Answer: Nakdong River, Response: Nakdong River, Eval: True. 381/869\n",
      "Correct Answer: Seoul, Response: Conquering Seoul., Eval: False. 382/869\n",
      "Correct Answer: copyright infringement, Response: Copyright infringement., Eval: True. 383/869\n",
      "Correct Answer: BitTorrent, Response: BitTorrent protocol, Eval: True. 384/869\n",
      "Correct Answer: Bronze Age collapse, Response: Bronze Age collapse, Eval: True. 385/869\n",
      "Correct Answer: 1.5 million, Response: 1.5 million, Eval: True. 386/869\n",
      "Correct Answer: 6.5 million, Response: 6.5 million, Eval: True. 387/869\n",
      "Correct Answer: Eternity and a Day, Response: Eternity and a Day, Eval: True. 388/869\n",
      "Correct Answer: Raízen, Response: Raízen, Eval: True. 389/869\n",
      "Correct Answer: blue whale, Response: Blue whale, Eval: True. 390/869\n",
      "Correct Answer: Sir Henry Middleton, Response: Sir Henry Middleton, Eval: True. 391/869\n",
      "Correct Answer: British, Response: Britain, Eval: True. 392/869\n",
      "Correct Answer: Different particles, Response: Particles, Eval: False. 393/869\n",
      "Correct Answer: Most referees are unnamed and essentially anonymous, though the WWE has let their officials reveal their names., Response: Referees, Eval: False. 394/869\n",
      "Correct Answer: magic, curses, the undead and Satanic imagery, Response: Magic, curses, the undead, Satanic imagery, Eval: True. 395/869\n",
      "Correct Answer: The speed of the emulsion, Response: Speed of the emulsion., Eval: True. 396/869\n",
      "Correct Answer: cameras that produce images only in JPEG format, Response: Cameras that produce images only in JPEG format., Eval: True. 397/869\n",
      "Correct Answer: destroyed during the siege of Tenochtitlan, Response: Destroyed during siege., Eval: True. 398/869\n",
      "Correct Answer: 1968, Response: 1968, Eval: True. 399/869\n",
      "Correct Answer: Leopoldo Méndez, Response: Leopoldo Méndez, Eval: True. 400/869\n",
      "Correct Answer: 2013, Response: 2013, Eval: True. 401/869\n",
      "Correct Answer: bubonic plague, Response: Bubonic plague, Eval: True. 402/869\n",
      "Correct Answer: Jérôme, Response: Jérôme, Eval: True. 403/869\n",
      "Correct Answer: The 5th Regiment, Response: The 5th Regiment., Eval: True. 404/869\n",
      "Correct Answer: Napoleon, Response: Napoleon, Eval: True. 405/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: the Roman Catholic Church and local rulers, Response: Roman Catholic Church and local rulers, Eval: True. 406/869\n",
      "Correct Answer: fourth largest, Response: Fourth, Eval: True. 407/869\n",
      "Correct Answer: The whale shark, Response: Whale shark, Eval: True. 408/869\n",
      "Correct Answer: July 18, 1896, Response: July 18, 1896., Eval: True. 409/869\n",
      "Correct Answer: Spencer W. Kimball Tower, Response: Spencer W. Kimball Tower, Eval: True. 410/869\n",
      "Correct Answer: 1784, Response: 1784, Eval: True. 411/869\n",
      "Correct Answer: 1877, Response: 1877, Eval: True. 412/869\n",
      "Correct Answer: trade secrets, Response: Trade secret, Eval: True. 413/869\n",
      "Correct Answer: the Seminoles lands west of the Mississippi River if they agreed to leave Florida. Many Seminole left at this time., Response: Lands west of the Mississippi River, Eval: False. 414/869\n",
      "Correct Answer: Humpy Bong, Response: Queen, Eval: False. 415/869\n",
      "Correct Answer: Bob Geldof and Midge Ure, Response: Bob Geldof and Midge Ure, Eval: True. 416/869\n",
      "Correct Answer: British, Response: British, Eval: True. 417/869\n",
      "Correct Answer: Book of Confessions, Response: Book of Confessions, Eval: True. 418/869\n",
      "Correct Answer: 750 Jews, Response: 750, Eval: True. 419/869\n",
      "Correct Answer: mined in the bordering Leipzig region, Response: Leipzig region., Eval: False. 420/869\n",
      "Correct Answer: Richard Dawkins, Response: Richard Dawkins, Eval: True. 421/869\n",
      "Correct Answer: 1961, Response: 1961, Eval: True. 422/869\n",
      "Correct Answer: Guardians of the Galaxy, Response: Guardians of the Galaxy, Eval: True. 423/869\n",
      "Correct Answer: French, Response: French, Eval: True. 424/869\n",
      "Correct Answer: the Ghadar Conspiracy, Response: Ghadar Conspiracy, Eval: True. 425/869\n",
      "Correct Answer: plants of medical importance, Response: Medicinal plants, Eval: True. 426/869\n",
      "Correct Answer: vegetation, Response: Vegetation, Eval: True. 427/869\n",
      "Correct Answer: woody plants, Response: Conifers, cycads, Ginkgo, and gnetophytes., Eval: False. 428/869\n",
      "Correct Answer: \"Into the Groove\", Response: \"Into the Groove\", Eval: True. 429/869\n",
      "Correct Answer: ABBA, Response: ABBA, Eval: True. 430/869\n",
      "Correct Answer: three months, Response: Three months, Eval: True. 431/869\n",
      "Correct Answer: the federal Constitution, Response: State constitution, Eval: True. 432/869\n",
      "Correct Answer: law, Response: law, Eval: True. 433/869\n",
      "Correct Answer: neolithic age domestication of plants and animals and the use of polished stone tools dating to sometime between 10,000 and 6,000 BC has been discovered, Response: Yes, Eval: False. 434/869\n",
      "Correct Answer: 1,200 mi, Response: 1,200 mi, Eval: True. 435/869\n",
      "Correct Answer: invitation to expatriates to return home to work for national development., Response: Yes., Eval: False. 436/869\n",
      "Correct Answer: ethnic groups constitute 32%, Response: Ethnic minorities, Eval: False. 437/869\n",
      "Correct Answer: 1948 and 1958, Response: 1948 to 1958, Eval: True. 438/869\n",
      "Correct Answer: South Plains, Response: South Plains, Eval: True. 439/869\n",
      "Correct Answer: data compression, Response: Data compression scheme., Eval: True. 440/869\n",
      "Correct Answer: senior sub-editor, Response: Senior sub-editor, Eval: True. 441/869\n",
      "Correct Answer: promotion of right-wing ministers to the cabinet, Response: EU, Eval: False. 442/869\n",
      "Correct Answer: Timothy Pontius, Response: Judge Timothy Pontius, Eval: True. 443/869\n",
      "Correct Answer: broad-spectrum herbicides, Response: Sulfonylureas, Eval: False. 444/869\n",
      "Correct Answer: Apple orchards were once plentiful, and Somerset is still a major producer of cider, Response: Apple orchids, Eval: False. 445/869\n",
      "Correct Answer: Boston, Response: Boston, Eval: True. 446/869\n",
      "Correct Answer: Local 34 of UNITE HERE, Response: Local 34 of UNITE HERE, Eval: True. 447/869\n",
      "Correct Answer: Henry Luce, Response: Henry Luce, Eval: True. 448/869\n",
      "Correct Answer: John Wycliffe, Response: John Wycliffe, Eval: True. 449/869\n",
      "Correct Answer: center for left-wing politics, Response: Left-wing politics, Eval: True. 450/869\n",
      "Correct Answer: WAAM 1600, Response: WAAM 1600, Eval: True. 451/869\n",
      "Correct Answer: polygonal, Response: Polygonal, Eval: True. 452/869\n",
      "Correct Answer: Paul Cézanne, Response: Paul Cézanne, Eval: True. 453/869\n",
      "Correct Answer: glass, Response: Glass, Eval: True. 454/869\n",
      "Correct Answer: The capability approach, Response: Capability approach, Eval: True. 455/869\n",
      "Correct Answer: 1,796, Response: 1,796, Eval: True. 456/869\n",
      "Correct Answer: overwhelmingly supported by the people of Norfolk Island, Response: overwhelmingly supported, Eval: False. 457/869\n",
      "Correct Answer: the Bank of England, Response: Bank of England, Eval: True. 458/869\n",
      "Correct Answer: Thoughts and Details on Scarcity, Response: Thoughts and Details on Scarcity, Eval: True. 459/869\n",
      "Correct Answer: copra, Response: Copra, Eval: True. 460/869\n",
      "Correct Answer: 1955, Response: 1955, Eval: True. 461/869\n",
      "Correct Answer: United Nations, Response: United Nations, Eval: True. 462/869\n",
      "Correct Answer: John XXIII, Response: John XXIII, Eval: True. 463/869\n",
      "Correct Answer: efficiency losses, Response: Efficiency losses., Eval: True. 464/869\n",
      "Correct Answer: power rating of the armature winding set, the speed of the machine, and the achievable air-gap flux density before core saturation, Response: Power rating, speed, achievable air-gap flux density., Eval: True. 465/869\n",
      "Correct Answer: 1999, Response: 1999, Eval: True. 466/869\n",
      "Correct Answer: 20th, Response: 20th, Eval: True. 467/869\n",
      "Correct Answer: higher, Response: Higher in urban areas., Eval: True. 468/869\n",
      "Correct Answer: their part in the decision-making, Response: Their role in the decision-making., Eval: True. 469/869\n",
      "Correct Answer: 1880, Response: 1880, Eval: True. 470/869\n",
      "Correct Answer: Greensboro Coliseum, Response: Greensboro Coliseum., Eval: True. 471/869\n",
      "Correct Answer: Joules, Response: Joules or kilocalories, Eval: False. 472/869\n",
      "Correct Answer: expensive, Response: Expensive., Eval: True. 473/869\n",
      "Correct Answer: nutritional content, Response: Reduced nutritional value., Eval: True. 474/869\n",
      "Correct Answer: 1853, Response: 1853, Eval: True. 475/869\n",
      "Correct Answer: the Russian defences, Response: The Russians, Eval: True. 476/869\n",
      "Correct Answer: The Crimean War, Response: Crimean War, Eval: True. 477/869\n",
      "Correct Answer: British Wellcome Trust, Response: Wellcome Trust, Eval: True. 478/869\n",
      "Correct Answer: D. Mitchell, Response: D. Mitchell, Eval: True. 479/869\n",
      "Correct Answer: the source of its own being without borrowed existence, Response: The necessary., Eval: False. 480/869\n",
      "Correct Answer: single character, Response: Characters, Eval: False. 481/869\n",
      "Correct Answer: Square-Block Characters, Response: Square-Block Characters, Eval: True. 482/869\n",
      "Correct Answer: Long Bay, Response: Long Bay, Horseshoe Bay, Khyber Pass, Eval: True. 483/869\n",
      "Correct Answer: Westminster system., Response: Westminster system., Eval: True. 484/869\n",
      "Correct Answer: at least 12,000, Response: 12,000, Eval: True. 485/869\n",
      "Correct Answer: development and harvesting by increased population, Response: Development and population growth., Eval: True. 486/869\n",
      "Correct Answer: the Ministry of Education, Response: Ministry of Education, Eval: True. 487/869\n",
      "Correct Answer: Surrounding the medieval core there is a ring of late 19th- and early 20th-century neighbourhoods, with newer neighbourhoods positioned farther out, Response: moat, Eval: False. 488/869\n",
      "Correct Answer: Germany, Response: Soviets, Eval: False. 489/869\n",
      "Correct Answer: no current can flow through the capacitor, Response: No current flows., Eval: True. 490/869\n",
      "Correct Answer: Glass, Response: Glass and mica capacitors., Eval: False. 491/869\n",
      "Correct Answer: Sumer, Response: Sumer (now Iraq), Eval: True. 492/869\n",
      "Correct Answer: mathematician, Response: Mathematician, Eval: True. 493/869\n",
      "Correct Answer: the germ theory of disease, Response: Joseph Lister's discoveries., Eval: False. 494/869\n",
      "Correct Answer: stronger and \"cool.\", Response: Stronger and \"cool\" designs., Eval: True. 495/869\n",
      "Correct Answer: Australia, Response: Australia, Eval: True. 496/869\n",
      "Correct Answer: Philadelphia, Pennsylvania, Response: Philadelphia, Pennsylvania., Eval: True. 497/869\n",
      "Correct Answer: The U.S. Olympic Network, Response: The U.S. Olympic Network, Eval: True. 498/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: Stop TB Partnership, Response: Stop TB Partnership, Eval: True. 499/869\n",
      "Correct Answer: Executive Order 8802, Response: Executive Order 8802, Eval: True. 500/869\n",
      "Correct Answer: an effort towards inclusion rather than a discriminatory practice, Response: Effort towards inclusion., Eval: True. 501/869\n",
      "Correct Answer: more than 60, Response: More than 60., Eval: True. 502/869\n",
      "Correct Answer: as soon as the final has finished, in order to be ready in time for the presentation ceremony., Response: After the final., Eval: False. 503/869\n",
      "Correct Answer: Governor's Foot Guard, Response: Governor's Foot Guard, Eval: True. 504/869\n",
      "Correct Answer: Not all of these small streams, Response: No., Eval: True. 505/869\n",
      "Correct Answer: Bobby Seale, Response: Bobby Seale, Eval: True. 506/869\n",
      "Correct Answer: The Beinecke Rare Book and Manuscript, Response: Beinecke Rare Book and Manuscript Library, Eval: True. 507/869\n",
      "Correct Answer: Grove Street Cemetery, Response: Grove Street Cemetery, Eval: True. 508/869\n",
      "Correct Answer: Route 34 extension, Response: Don't know., Eval: False. 509/869\n",
      "Correct Answer: Philippe Richert, Response: Philippe Richert, Eval: True. 510/869\n",
      "Correct Answer: Panjim, Response: Panjim, Eval: True. 511/869\n",
      "Correct Answer: Saturday, Response: Saturday, Eval: True. 512/869\n",
      "Correct Answer: Protestant churches, Response: Protestant churches, Eval: True. 513/869\n",
      "Correct Answer: work such as those by Amish children, Response: Yes., Eval: False. 514/869\n",
      "Correct Answer: 1998, Response: 1988, Eval: False. 515/869\n",
      "Correct Answer: claimed that it was at a loss to understand the allegations, Response: Agriprocessors claimed to be at a loss to understand the allegations., Eval: True. 516/869\n",
      "Correct Answer: General George Washington, Response: George Washington, Eval: True. 517/869\n",
      "Correct Answer: eastern coastal plain, Response: Eastern coastal plain, especially at port cities such as Wilmington and New Bern., Eval: True. 518/869\n",
      "Correct Answer: millions, Response: $119 million, Eval: False. 519/869\n",
      "Correct Answer: 19, Response: 19, Eval: True. 520/869\n",
      "Correct Answer: \"big book\" on Natural Selection,, Response: \"big book\" on Natural Selection, Eval: True. 521/869\n",
      "Correct Answer: variability and change in the environment, Response: Variability and change in the environment., Eval: True. 522/869\n",
      "Correct Answer: Communist Party, Response: Communist Party of the Soviet Union, Eval: True. 523/869\n",
      "Correct Answer: militia, Response: Authorities, Eval: False. 524/869\n",
      "Correct Answer: Rafiq Nishonov, Response: Rafiq Nishonov, Eval: True. 525/869\n",
      "Correct Answer: about 70 AD, Response: 70 AD, Eval: True. 526/869\n",
      "Correct Answer: lunar, Response: Lunar eclipse., Eval: True. 527/869\n",
      "Correct Answer: trial courts of general jurisdiction, Response: Trial courts of general jurisdiction., Eval: True. 528/869\n",
      "Correct Answer: scribes tended to add words, for clarification or out of habit, Response: The main principle of lectio brevior is shorter reading., Eval: False. 529/869\n",
      "Correct Answer: 1991, Response: 1991, Eval: True. 530/869\n",
      "Correct Answer: bad reputation, Response: Destroyed by jukeboxes., Eval: False. 531/869\n",
      "Correct Answer: visual cue to DJs mixing the records, Response: Visual cue for DJs., Eval: True. 532/869\n",
      "Correct Answer: 1925, Response: 1925, Eval: True. 533/869\n",
      "Correct Answer: Time Warner, Response: Warner Bros., Eval: False. 534/869\n",
      "Correct Answer: six, Response: Six, Eval: True. 535/869\n",
      "Correct Answer: Śvetāśvatara Upanishad, Response: Śvetāśvatara Upanishad, Eval: True. 536/869\n",
      "Correct Answer: Lloyd George, Response: Lloyd George, Eval: True. 537/869\n",
      "Correct Answer: S.P.E.B.S.Q.S.A, Response: S.P.E.B.S.Q.S.A., Eval: True. 538/869\n",
      "Correct Answer: the Netherlands, Response: Germany, Eval: False. 539/869\n",
      "Correct Answer: Margarette Reglerin, Response: Margarette Reglerin, Eval: True. 540/869\n",
      "Correct Answer: mourning for the death of George III, Response: Don't know., Eval: False. 541/869\n",
      "Correct Answer: John Gurdon, Response: John Gurdon, Eval: True. 542/869\n",
      "Correct Answer: Ford Motor Company, Response: Ford, Eval: True. 543/869\n",
      "Correct Answer: 585, Response: 411, Eval: False. 544/869\n",
      "Correct Answer: Catholic Church, Response: Galician Catholic Church, Eval: False. 545/869\n",
      "Correct Answer: Converters, Response: USB-to-PS/2 adapter, Eval: False. 546/869\n",
      "Correct Answer: that devices connect in a low-power mode (100 mA maximum) and communicate their current requirements to the host, Response: Low-power mode (100 mA maximum), Eval: False. 547/869\n",
      "Correct Answer: subarctic climate, Response: Subarctic climate., Eval: True. 548/869\n",
      "Correct Answer: International Organization for Standardization, Response: International Organization for Standardization, Eval: True. 549/869\n",
      "Correct Answer: auto, Response: Auto industry, Eval: True. 550/869\n",
      "Correct Answer: Detroit Opera House, Response: Detroit Opera House, Eval: True. 551/869\n",
      "Correct Answer: Detroit techno, Response: Techno, Eval: False. 552/869\n",
      "Correct Answer: St. Joseph's, Response: St. Joseph's., Eval: True. 553/869\n",
      "Correct Answer: the mid-1960s, Response: 1960s, Eval: False. 554/869\n",
      "Correct Answer: Neasden Temple, Response: Neasden Temple, Eval: True. 555/869\n",
      "Correct Answer: Andrew Lloyd Webber, Response: Andrew Lloyd Webber, Eval: True. 556/869\n",
      "Correct Answer: Diffusion of innovations, Response: Diffusion of innovations theory, Eval: True. 557/869\n",
      "Correct Answer: five hundred species, Response: 500, Eval: True. 558/869\n",
      "Correct Answer: Law Professor Frederick Mark Gedicks, Response: Frederick Mark Gedicks, Eval: True. 559/869\n",
      "Correct Answer: Princess Wencheng, Response: Princess Wencheng, Eval: True. 560/869\n",
      "Correct Answer: Tibetan or Sanskrit, Response: Tibetan, Sanskrit, Eval: True. 561/869\n",
      "Correct Answer: one, Response: One game., Eval: True. 562/869\n",
      "Correct Answer: Approximately 44, Response: 44%, Eval: True. 563/869\n",
      "Correct Answer: perjury related to betting on their own games, Response: Perjury related to betting., Eval: False. 564/869\n",
      "Correct Answer: Raymond Poincaré, Response: Raymond Poincaré, Eval: True. 565/869\n",
      "Correct Answer: 2.1 to 2.2 miles, Response: 2.1 to 2.2 miles, Eval: True. 566/869\n",
      "Correct Answer: fifth, Response: Fifth, Eval: True. 567/869\n",
      "Correct Answer: Route 66, Response: Route 66, Eval: True. 568/869\n",
      "Correct Answer: 7000–9000 BCE, Response: 7000–9000 BCE, Eval: True. 569/869\n",
      "Correct Answer: Khārabēḷa, Response: Khārabēḷa, Eval: True. 570/869\n",
      "Correct Answer: Central Asia, Response: Central Asia, Eval: True. 571/869\n",
      "Correct Answer: Lord Curzon, Response: Lord Curzon, Eval: True. 572/869\n",
      "Correct Answer: instructor, Response: Instructor at Cairo Royal Military Academy., Eval: True. 573/869\n",
      "Correct Answer: single-party system, Response: Single-party system under the National Union., Eval: True. 574/869\n",
      "Correct Answer: worsening, Response: Worsening, nationalized., Eval: False. 575/869\n",
      "Correct Answer: March 1969, Response: March 1969., Eval: True. 576/869\n",
      "Correct Answer: 1925–35, Response: 1925, Eval: False. 577/869\n",
      "Correct Answer: the second, Response: Second, Eval: True. 578/869\n",
      "Correct Answer: The level of activity in the brain of neurotransmitters, Response: Activity in neurotransmitters, Eval: False. 579/869\n",
      "Correct Answer: €212.1bn, Response: €212.1bn, Eval: True. 580/869\n",
      "Correct Answer: hotel rooms and office space, Response: Hotel rooms and office space., Eval: True. 581/869\n",
      "Correct Answer: Margittai Neumann János, Response: János, Eval: False. 582/869\n",
      "Correct Answer: p represents the prices of the goods, Response: prices of the goods, Eval: True. 583/869\n",
      "Correct Answer: Metal Gear Solid 4: Guns of the Patriots, Response: Metal Gear Solid 4: Guns of the Patriots, Eval: True. 584/869\n",
      "Correct Answer: real, Response: Real currency, Eval: True. 585/869\n",
      "Correct Answer: the original PS3, Response: The original \"fat\" model., Eval: False. 586/869\n",
      "Correct Answer: 1765, Response: 1765, Eval: True. 587/869\n",
      "Correct Answer: Composing two of these symmetry functions, Response: Function composition., Eval: True. 588/869\n",
      "Correct Answer: 4.7 million, Response: 4.7 million, Eval: True. 589/869\n",
      "Correct Answer: tsetse fly, Response: Tsetse fly., Eval: True. 590/869\n",
      "Correct Answer: decrease the number of people hospitalized for asthma, Response: Decrease hospitalizations for asthma., Eval: True. 591/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: LaserDiscs, Response: LaserDiscs use analog video., Eval: True. 592/869\n",
      "Correct Answer: David Bowie, Response: David Bowie, Eval: True. 593/869\n",
      "Correct Answer: division of powers between two levels of government of equal status, Response: Yes., Eval: True. 594/869\n",
      "Correct Answer: federal structure, Response: Consent, Eval: False. 595/869\n",
      "Correct Answer: arthropods, Response: Arthropods, Eval: True. 596/869\n",
      "Correct Answer: deuterostomes, Response: Deuterostomes, Eval: True. 597/869\n",
      "Correct Answer: \"a universe with a god would be a completely different kind of universe from one without, and it would be a scientific difference.\", Response: Empirical question., Eval: False. 598/869\n",
      "Correct Answer: 12, Response: 12, Eval: True. 599/869\n",
      "Correct Answer: November 2002, Response: November 2002, Eval: True. 600/869\n",
      "Correct Answer: deeply unpopular, Response: Unpopular., Eval: True. 601/869\n",
      "Correct Answer: the early 1960s, Response: early 1960s, Eval: True. 602/869\n",
      "Correct Answer: the gateway between East and West, Response: Gateway between East and West, Eval: True. 603/869\n",
      "Correct Answer: four, Response: Four, Eval: True. 604/869\n",
      "Correct Answer: 30–60¢ higher, Response: 30-60¢ higher, Eval: True. 605/869\n",
      "Correct Answer: bibliophile, Response: Popper inherited a disposition for books., Eval: False. 606/869\n",
      "Correct Answer: possible framework, Response: Metaphysical research program., Eval: False. 607/869\n",
      "Correct Answer: 1815, Response: 1815, Eval: True. 608/869\n",
      "Correct Answer: Japan, Response: Japan, Eval: True. 609/869\n",
      "Correct Answer: flowering plants, Response: flowering plants, Eval: True. 610/869\n",
      "Correct Answer: thorax, Response: Tergum, Eval: False. 611/869\n",
      "Correct Answer: Diptera, Response: Fungus gnats, Eval: False. 612/869\n",
      "Correct Answer: beneficial, Response: Beneficial, Eval: True. 613/869\n",
      "Correct Answer: Dean Amadon, Response: Dean Amadon, Eval: True. 614/869\n",
      "Correct Answer: the American Journal of Human Genetics, Response: American Journal of Human Genetics, Eval: True. 615/869\n",
      "Correct Answer: medical conditions, Response: Medical conditions, Eval: True. 616/869\n",
      "Correct Answer: 12,884, Response: 12,884, Eval: True. 617/869\n",
      "Correct Answer: five, Response: Five, Eval: True. 618/869\n",
      "Correct Answer: 1889, Response: 1889, Eval: True. 619/869\n",
      "Correct Answer: Paris-Le Bourget, Response: Paris-Le Bourget, Eval: True. 620/869\n",
      "Correct Answer: Typhon, Response: Typhon, Eval: True. 621/869\n",
      "Correct Answer: late archaic period, Response: Late Archaic period, Eval: True. 622/869\n",
      "Correct Answer: joking at one point about one of Kerry's remarks, \"That answer made me want to scowl.\", Response: Joked about Kerry's remarks., Eval: False. 623/869\n",
      "Correct Answer: the Whitlam government was dismissed by the Governor-General, Sir John Kerr, Response: Whitlam government dismissed., Eval: False. 624/869\n",
      "Correct Answer: the Edo period, Response: Edo period, Eval: True. 625/869\n",
      "Correct Answer: Toyotomi Hideyoshi, Response: Toyotomi Hideyoshi, Eval: True. 626/869\n",
      "Correct Answer: seeing the source code, Response: Knowledge of internal implementation., Eval: False. 627/869\n",
      "Correct Answer: 65%, Response: 65%, Eval: True. 628/869\n",
      "Correct Answer: soda-lime glass, Response: Soda-lime glass., Eval: True. 629/869\n",
      "Correct Answer: an electrically neutral particle, Response: Photon, Eval: True. 630/869\n",
      "Correct Answer: either-or nature of uncertainty, Response: Uncertainty, Eval: False. 631/869\n",
      "Correct Answer: 6%, Response: 6%, Eval: True. 632/869\n",
      "Correct Answer: Corso Calatifimi, Response: Corso Calatifimi, Eval: True. 633/869\n",
      "Correct Answer: minerals, Response: Minerals, Eval: True. 634/869\n",
      "Correct Answer: Zinc, Response: Zinc, Eval: True. 635/869\n",
      "Correct Answer: chemotherapeutic, Response: chemotherapeutic agent, Eval: True. 636/869\n",
      "Correct Answer: Late Baroque architecture, Response: Late Baroque, Eval: True. 637/869\n",
      "Correct Answer: imperfective, Response: Imperfective., Eval: True. 638/869\n",
      "Correct Answer: July 17, 2012, Response: July 17, 2012, Eval: True. 639/869\n",
      "Correct Answer: translation services, Response: Translation services, Eval: True. 640/869\n",
      "Correct Answer: available energy, Response: Available energy, Eval: True. 641/869\n",
      "Correct Answer: Virtual photons, Response: Virtual photons, Eval: True. 642/869\n",
      "Correct Answer: Provinzialverband, Response: Provinzialverband, Eval: True. 643/869\n",
      "Correct Answer: Nevşehirli Damat İbrahim Pasha, Response: Nevşehirli Damat İbrahim Pasha, Eval: True. 644/869\n",
      "Correct Answer: Sultan Abdülhamid II, Response: Sultan Abdülhamid II, Eval: True. 645/869\n",
      "Correct Answer: the wide ethnic range of the Ottoman Empire, Response: Multitude of influences., Eval: False. 646/869\n",
      "Correct Answer: the surface of the planet, Response: Surface of the planet., Eval: True. 647/869\n",
      "Correct Answer: Neolithic pastoralists, Response: Neolithic pastoralists, Eval: True. 648/869\n",
      "Correct Answer: sick of Hutchins' meddling, Response: Departmental autonomy, Eval: False. 649/869\n",
      "Correct Answer: good men, Response: Good men for running it, Eval: False. 650/869\n",
      "Correct Answer: irritable bowel syndrome, Response: Irritable bowel syndrome., Eval: True. 651/869\n",
      "Correct Answer: licence to teach, Response: Qualification to teach, Eval: False. 652/869\n",
      "Correct Answer: anti-Americanism and radical extremism, Response: Places of radical revivalism., Eval: False. 653/869\n",
      "Correct Answer: 8th, Response: 8th, Eval: True. 654/869\n",
      "Correct Answer: Orlando International Airport, Response: Orlando International Airport, Eval: True. 655/869\n",
      "Correct Answer: 43.4%, Response: 43.4%, Eval: True. 656/869\n",
      "Correct Answer: five, Response: 5, Eval: True. 657/869\n",
      "Correct Answer: Admiral Zumwalt, Response: Kerry, Eval: False. 658/869\n",
      "Correct Answer: William Weld, Response: William Weld, Eval: True. 659/869\n",
      "Correct Answer: Massachusetts, Response: Rhode Island, Eval: False. 660/869\n",
      "Correct Answer: 38.55%, Response: 38.55%, Eval: True. 661/869\n",
      "Correct Answer: 20 large hotels, Response: Over 20., Eval: False. 662/869\n",
      "Correct Answer: relations of ideas, Response: Relations of ideas, Eval: True. 663/869\n",
      "Correct Answer: metaphysical and epistemological, Response: metaphysical and epistemological dualist, Eval: True. 664/869\n",
      "Correct Answer: Latin and German, Response: Latin and German, Eval: True. 665/869\n",
      "Correct Answer: with nominative case, Response: Nominative case., Eval: True. 666/869\n",
      "Correct Answer: international Millennium Development Goals, Response: International Millennium Development Goals, Eval: True. 667/869\n",
      "Correct Answer: Memphis, Response: Memphis, Eval: True. 668/869\n",
      "Correct Answer: beef cattle, Response: Cattle, Eval: True. 669/869\n",
      "Correct Answer: high cultural references of 1960s rock artists, Response: Traditionalist, hegemonic, rockist aesthetics., Eval: False. 670/869\n",
      "Correct Answer: an abrasive, confrontational and nihilistic, Response: Abrasive, confrontational, nihilistic, Eval: True. 671/869\n",
      "Correct Answer: Canadian Football Hall of Fame, Response: Canadian Football Hall of Fame, Eval: True. 672/869\n",
      "Correct Answer: some individual Caribbean islands in the West Indies,, Response: Some individual Caribbean islands., Eval: True. 673/869\n",
      "Correct Answer: a column of 18,000 men, Response: Command of the Duke of Brunswick-Bevern., Eval: False. 674/869\n",
      "Correct Answer: Pitt now prepared to send troops into Germany;, Response: Pitt, Eval: False. 675/869\n",
      "Correct Answer: humor, Response: Sense of humor, Eval: True. 676/869\n",
      "Correct Answer: the chosen people, Response: Chosen people., Eval: True. 677/869\n",
      "Correct Answer: a pro-western monarch, Idris, who banned political parties and established an absolute monarchy., Response: Absolute monarchy, Eval: False. 678/869\n",
      "Correct Answer: One September Revolution, Response: One September Revolution, Eval: True. 679/869\n",
      "Correct Answer: Soviet Union, Response: Soviet Union, Eval: True. 680/869\n",
      "Correct Answer: Algiers, Response: Algiers, Eval: True. 681/869\n",
      "Correct Answer: professional, Response: Professional, Eval: True. 682/869\n",
      "Correct Answer: enosis, Response: Enosis, Eval: True. 683/869\n",
      "Correct Answer: Sunni Islam, Response: Sunni Islam, Eval: True. 684/869\n",
      "Correct Answer: Two, Response: Two, Eval: True. 685/869\n",
      "Correct Answer: 2054, Response: 2054, Eval: True. 686/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: Allen Daviau, Response: Allen Daviau, Eval: True. 687/869\n",
      "Correct Answer: the controller, Response: Traction machine., Eval: False. 688/869\n",
      "Correct Answer: underground public elevator, Response: 3-station underground public elevator, Eval: False. 689/869\n",
      "Correct Answer: hydrogen and helium, Response: Hydrogen and helium., Eval: True. 690/869\n",
      "Correct Answer: southern cyclonic storm, Response: The Scooter is a storm., Eval: False. 691/869\n",
      "Correct Answer: overhead wires, Response: Both third rail and overhead wires are used., Eval: False. 692/869\n",
      "Correct Answer: electric, Response: Electric trains, Eval: True. 693/869\n",
      "Correct Answer: Spanish is the most widely taught language in American colleges and universities with 53 percent of the total number of people enrolled, Response: Spanish language classes are popular in the U.S., Eval: True. 694/869\n",
      "Correct Answer: 1970, Response: 1970, Eval: True. 695/869\n",
      "Correct Answer: 300, Response: 300, Eval: True. 696/869\n",
      "Correct Answer: prescribed targets are not hit, Response: Prescribed targets not hit., Eval: True. 697/869\n",
      "Correct Answer: 138,000, Response: 138,000, Eval: True. 698/869\n",
      "Correct Answer: no less than 667, Response: 667, Eval: True. 699/869\n",
      "Correct Answer: Liverpool, Response: Liverpool, Eval: True. 700/869\n",
      "Correct Answer: the animals on the endangered species list still were not getting enough protection, thus further threatening their extinction, Response: Expanded the endangered species program., Eval: False. 701/869\n",
      "Correct Answer: vacuus, Response: vacuus, Eval: True. 702/869\n",
      "Correct Answer: due to space weather, Response: Space weather., Eval: True. 703/869\n",
      "Correct Answer: Chang'an, Response: Chang'an., Eval: True. 704/869\n",
      "Correct Answer: criminals were never imprisoned, Response: Not imprisoned., Eval: False. 705/869\n",
      "Correct Answer: rammed-earth, Response: Rammed-earth walls., Eval: True. 706/869\n",
      "Correct Answer: I'jaz, Response: I'jaz, Eval: True. 707/869\n",
      "Correct Answer: Kufa, Response: Kufa, Eval: True. 708/869\n",
      "Correct Answer: Mount Rainier, Response: Mount Baker, Eval: False. 709/869\n",
      "Correct Answer: spiral, Response: Spiral pattern., Eval: True. 710/869\n",
      "Correct Answer: in modern devices, Response: 65 nm technology node., Eval: False. 711/869\n",
      "Correct Answer: Beijing was captured by Li Zicheng's peasant rebels, Response: To avoid capture., Eval: False. 712/869\n",
      "Correct Answer: in 1898, Response: 1898, Eval: True. 713/869\n",
      "Correct Answer: transportation and communication infrastructure, Response: Transportation and communication infrastructure, corporations, managerial revolution., Eval: False. 714/869\n",
      "Correct Answer: social and societal upheaval, Response: A period of prosperity and cultural dynamism., Eval: False. 715/869\n",
      "Correct Answer: legal contracts, marriages, and criminal judgments, Response: Recognition of legal contracts, marriages, and criminal judgments., Eval: True. 716/869\n",
      "Correct Answer: over 1 million, Response: Over 1 million., Eval: True. 717/869\n",
      "Correct Answer: small, Response: Small antenna, Eval: True. 718/869\n",
      "Correct Answer: Rr / (Rr + Rloss), Response: Efficiency = Rr / (Rr + Rloss), Eval: True. 719/869\n",
      "Correct Answer: lobes, Response: Number of lobes, Eval: True. 720/869\n",
      "Correct Answer: paraphyletic, Response: primitive species, Eval: False. 721/869\n",
      "Correct Answer: 973 mi, Response: 973 miles, Eval: True. 722/869\n",
      "Correct Answer: 30%, Response: 30%, Eval: True. 723/869\n",
      "Correct Answer: beachfront, Response: Beachfront city., Eval: True. 724/869\n",
      "Correct Answer: Buster Keaton's Spite Marriage, Response: Buster Keaton's Spite Marriage (1929), Eval: True. 725/869\n",
      "Correct Answer: 1988, Response: 1988, Eval: True. 726/869\n",
      "Correct Answer: between 200 and 300, Response: 200-300, Eval: True. 727/869\n",
      "Correct Answer: morphine, Response: Morphine injection., Eval: True. 728/869\n",
      "Correct Answer: unclear, Response: Unclear., Eval: True. 729/869\n",
      "Correct Answer: see database information in a more business-related way, Response: Different views for different departments., Eval: False. 730/869\n",
      "Correct Answer: 118, Response: 118 miles, Eval: True. 731/869\n",
      "Correct Answer: Oro Valley, Response: Oro Valley, Eval: True. 732/869\n",
      "Correct Answer: 2009, Response: 2009, Eval: True. 733/869\n",
      "Correct Answer: marked bike routes, signal crossings, on-street bike lanes, mountain-biking trails, and dedicated shared-use paths, Response: Bike routes, bike lanes, mountain-biking trails, shared-use paths, and The Loop., Eval: False. 734/869\n",
      "Correct Answer: 1967, Response: 1967, Eval: True. 735/869\n",
      "Correct Answer: Saint Mesrop Mashtots, Response: Saint Mesrop Mashtots, Eval: True. 736/869\n",
      "Correct Answer: in the cytoplasm, Response: Cytoplasm/nucleoid., Eval: True. 737/869\n",
      "Correct Answer: amphitrichous, Response: Amphitrichous., Eval: True. 738/869\n",
      "Correct Answer: numerically controlled drill, Response: Numerically Controlled Drill, Eval: True. 739/869\n",
      "Correct Answer: a connection, Response: A wire., Eval: False. 740/869\n",
      "Correct Answer: the three most important centres of Greek learning were situated in Chios, Smyrna and Aivali, all three major centres of Greek commerce, Response: Chios, Smyrna, Aivali, Eval: True. 741/869\n",
      "Correct Answer: the percentages for female students being higher than for male, Response: Don't know., Eval: False. 742/869\n",
      "Correct Answer: The meeting was to pave the way for a break away from The Football League., Response: To pave the way for a break away from The Football League., Eval: True. 743/869\n",
      "Correct Answer: La Liga, Response: La Liga, Eval: True. 744/869\n",
      "Correct Answer: critics maintain that the payments actually widen the gap between teams that have reached the Premier League and those that have not,, Response: Critics say it widens the gap., Eval: True. 745/869\n",
      "Correct Answer: appointed magistrates, Response: Magistrates, Eval: False. 746/869\n",
      "Correct Answer: more than 500,000, Response: 500,000, Eval: False. 747/869\n",
      "Correct Answer: Sulla, Response: Sulla, Eval: True. 748/869\n",
      "Correct Answer: magistrates, Response: Consuls and praetors, Eval: False. 749/869\n",
      "Correct Answer: Germany and Italy, Response: Germany, Italy, Japan, Eval: False. 750/869\n",
      "Correct Answer: aircraft carriers, Response: Aircraft carriers., Eval: True. 751/869\n",
      "Correct Answer: Imperial Japanese Navy, Response: Doctrine of fleet battles., Eval: True. 752/869\n",
      "Correct Answer: battle of Iwo Jima, Response: Battle in Iwo Jima., Eval: True. 753/869\n",
      "Correct Answer: Lieutenant Gray, Response: Lieutenant Gray, Eval: True. 754/869\n",
      "Correct Answer: 2011, Response: 2011, Eval: True. 755/869\n",
      "Correct Answer: give direct access to Tijuana International Airport, Response: To give direct access to Tijuana International Airport., Eval: True. 756/869\n",
      "Correct Answer: Organisation of Islamic Cooperation and the Islamic Educational, Scientific and Cultural Organization, Response: Organisation of Islamic Cooperation, Islamic Educational, Scientific and Cultural Organization, Eval: True. 757/869\n",
      "Correct Answer: the Hellenistic Seleucid Empire, Response: Seleucid Empire, Eval: True. 758/869\n",
      "Correct Answer: Department of Environment, Response: Department of Environment, Eval: True. 759/869\n",
      "Correct Answer: Over 80%, Response: Over 80%, Eval: True. 760/869\n",
      "Correct Answer: Hadrian's Wall, Response: Hadrian's Wall, Eval: True. 761/869\n",
      "Correct Answer: 1871, Response: 1871, Eval: True. 762/869\n",
      "Correct Answer: individual football associations, Response: Individual football associations., Eval: True. 763/869\n",
      "Correct Answer: 1980, Response: 1980, Eval: True. 764/869\n",
      "Correct Answer: textile arts and quilting, Response: Textile arts and quilting, Eval: True. 765/869\n",
      "Correct Answer: make metaphysical assumptions about how the universe works, Response: Re-imagine basic assumptions., Eval: False. 766/869\n",
      "Correct Answer: causal efficacy (or \"physical prehension\") and presentational immediacy (or \"conceptual prehension\"), Response: Causal efficacy and presentational immediacy., Eval: True. 767/869\n",
      "Correct Answer: It has been severely criticized, Response: Severely criticized., Eval: True. 768/869\n",
      "Correct Answer: mechanism of action, Response: Target specificity, Eval: False. 769/869\n",
      "Correct Answer: Steven Sinofsky, Response: Steven Sinofsky, Eval: True. 770/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: display constantly updated information and content, Response: Display constantly updated information., Eval: False. 771/869\n",
      "Correct Answer: Common Market for Eastern and Southern Africa, Response: Common Market for Eastern and Southern Africa, Eval: True. 772/869\n",
      "Correct Answer: Botswana, Lesotho and Swaziland, Response: Botswana, Lesotho, Swaziland, Eval: True. 773/869\n",
      "Correct Answer: on-demand human-translation services, Response: On-demand human translation., Eval: True. 774/869\n",
      "Correct Answer: slope-line approach system, Response: Slope-line approach system., Eval: True. 775/869\n",
      "Correct Answer: 10th century, Response: 10th century, Eval: True. 776/869\n",
      "Correct Answer: US$199, Response: $199, Eval: True. 777/869\n",
      "Correct Answer: marshland, Response: Eastern Arabia, Eval: False. 778/869\n",
      "Correct Answer: outside the city walls in graveyards, Response: Outside the city walls in graveyards., Eval: True. 779/869\n",
      "Correct Answer: Salai, Response: Salai, Eval: True. 780/869\n",
      "Correct Answer: 11 years, Response: 11 years, Eval: True. 781/869\n",
      "Correct Answer: Funafuti Lagoon, Response: Fresh-water aquifer, Eval: False. 782/869\n",
      "Correct Answer: published Esame Critico sulla dottrina dell’ Angelico Dottore S. Tommaso di Aquino circa il Peccato originale, relativamente alla Beatissima Vergine Maria, Response: Published a critical examination of Aquinas' doctrine on original sin and the Virgin Mary., Eval: True. 783/869\n",
      "Correct Answer: 1946, Response: 1946, Eval: True. 784/869\n",
      "Correct Answer: 73,464, Response: 14%, Eval: False. 785/869\n",
      "Correct Answer: 1867, Response: 1867, Eval: True. 786/869\n",
      "Correct Answer: precise and reliable nuclear operations, Response: To achieve a high standard of protection through a stringent nuclear surety program., Eval: False. 787/869\n",
      "Correct Answer: Regional Combatant Commander, Response: Secretary of Defense (SECDEF), Eval: False. 788/869\n",
      "Correct Answer: General Norton A. Schwartz, Response: General Norton A. Schwartz, Eval: True. 789/869\n",
      "Correct Answer: lower color rendering, Response: Trade-off between efficacy and color rendering., Eval: False. 790/869\n",
      "Correct Answer: Barcode scanners, Response: Barcode scanners, Eval: True. 791/869\n",
      "Correct Answer: Birds, Response: Birds, Eval: True. 792/869\n",
      "Correct Answer: bony knob on the alular metacarpal, Response: Bony knob on alular metacarpal., Eval: True. 793/869\n",
      "Correct Answer: Cooperative breeding, Response: Cooperative breeding, Eval: True. 794/869\n",
      "Correct Answer: viceroys, Response: Liaodong Han Bannermen, Eval: False. 795/869\n",
      "Correct Answer: Rehe, Response: Rehe, Eval: True. 796/869\n",
      "Correct Answer: Manchu and Mongol, Response: Manchu and Mongol., Eval: True. 797/869\n",
      "Correct Answer: oral history and through archaeological investigations, Response: Oral history and archaeological investigations., Eval: True. 798/869\n",
      "Correct Answer: 66.4%, Response: 66.4%, Eval: True. 799/869\n",
      "Correct Answer: destabilize the country, Response: European plans to destabilize the country., Eval: True. 800/869\n",
      "Correct Answer: carmine, Response: carmine, Eval: True. 801/869\n",
      "Correct Answer: south, Response: South, Eval: True. 802/869\n",
      "Correct Answer: 525 BC to 402 BC,, Response: 525 BC to 402 BC, Eval: True. 803/869\n",
      "Correct Answer: lack of fair trials, that often last only a few minutes, Response: Lack of fair trials., Eval: True. 804/869\n",
      "Correct Answer: Fried onions, Response: Fried onions., Eval: True. 805/869\n",
      "Correct Answer: Chios, Response: Chios, Eval: True. 806/869\n",
      "Correct Answer: Hungary, Response: Kingdom of Hungary, Eval: True. 807/869\n",
      "Correct Answer: Lisbon, Response: Lisbon, Eval: True. 808/869\n",
      "Correct Answer: varies widely, Response: Varies widely., Eval: True. 809/869\n",
      "Correct Answer: Jupiter, Response: Jupiter, Eval: True. 810/869\n",
      "Correct Answer: rex sacrorum, Response: rex sacrorum, Eval: True. 811/869\n",
      "Correct Answer: magi, Response: Magi, Eval: True. 812/869\n",
      "Correct Answer: Decius, Response: Decius, Eval: True. 813/869\n",
      "Correct Answer: \"video response\", Response: Video response, Eval: True. 814/869\n",
      "Correct Answer: block, Response: moderate and block comments, Eval: True. 815/869\n",
      "Correct Answer: Board of Education of Kiryas Joel Village School District v. Grumet, Response: Board of Education of Kiryas Joel Village School District v. Grumet (1994), Eval: True. 816/869\n",
      "Correct Answer: an alliance of unlikely partners, Response: An alliance of unlikely partners., Eval: True. 817/869\n",
      "Correct Answer: France, Response: France, Eval: True. 818/869\n",
      "Correct Answer: International Lutheran Council and the Confessional Evangelical Lutheran Conference, Response: International Lutheran Council, Confessional Evangelical Lutheran Conference, Eval: True. 819/869\n",
      "Correct Answer: 1523, Response: 1523, Eval: True. 820/869\n",
      "Correct Answer: Câmara Legislativa, Response: Câmara Legislativa, Eval: True. 821/869\n",
      "Correct Answer: OTE, Response: OTE, Eval: True. 822/869\n",
      "Correct Answer: 764, Response: 764 km, Eval: True. 823/869\n",
      "Correct Answer: Democratic Congressional Campaign Committee, Response: Democratic Congressional Campaign Committee, Eval: True. 824/869\n",
      "Correct Answer: Eastern, Response: Eastern Armenian, Eval: True. 825/869\n",
      "Correct Answer: 144,000, Response: 144,000, Eval: True. 826/869\n",
      "Correct Answer: a worldwide brotherhood, Response: Worldwide brotherhood, Eval: True. 827/869\n",
      "Correct Answer: January 17, 1961, Response: January 17, 1961, Eval: True. 828/869\n",
      "Correct Answer: February 1943, Response: February 1943, Eval: True. 829/869\n",
      "Correct Answer: Herbert Brownell, Response: Herbert Brownell, Eval: True. 830/869\n",
      "Correct Answer: May 1, 1960, Response: May 1, 1960, Eval: True. 831/869\n",
      "Correct Answer: President's Private Secretary, Response: President's Private Secretary, Eval: True. 832/869\n",
      "Correct Answer: the last patch of the original hemlock forest that once covered the entire county, Response: The last patch of the original hemlock forest., Eval: False. 833/869\n",
      "Correct Answer: 2006, Response: 2006, Eval: True. 834/869\n",
      "Correct Answer: major global financial institutions, Response: Major global financial institutions., Eval: True. 835/869\n",
      "Correct Answer: 60%, Response: 60%, Eval: True. 836/869\n",
      "Correct Answer: close to $2 trillion, Response: $2 trillion, Eval: True. 837/869\n",
      "Correct Answer: loss of close to $6 trillion in housing wealth, Response: Loss of housing and stock wealth., Eval: False. 838/869\n",
      "Correct Answer: Gallaecia, Response: Gallaecia, Eval: True. 839/869\n",
      "Correct Answer: 1933, Response: 1933, Eval: True. 840/869\n",
      "Correct Answer: Grupo RAR's Vitacress, Sovena, Lactogal, Vale da Rosa, Companhia das Lezírias and Valouro, Response: Companies like Grupo RAR's Vitacress, Sovena, Lactogal, Vale da Rosa, Companhia das Lezírias, and Valouro., Eval: True. 841/869\n",
      "Correct Answer: Atlantic Ocean, Response: Atlantic Ocean, Eval: True. 842/869\n",
      "Correct Answer: human beings, Response: Critical thinking and evidence., Eval: False. 843/869\n",
      "Correct Answer: Jacques Hébert, Response: Jacques Hébert, Eval: False. 844/869\n",
      "Correct Answer: raising eustatic sea levels, Response: Raised eustatic sea levels., Eval: True. 845/869\n",
      "Correct Answer: 1800, Response: 1800, Eval: True. 846/869\n",
      "Correct Answer: authorize a search of a suspect's house, Response: Authorize a search of a suspect's house., Eval: True. 847/869\n",
      "Correct Answer: ethnic cleansing, Response: ethnic cleansing, Eval: True. 848/869\n",
      "Correct Answer: Sea grapes, Response: Cacti, Eval: False. 849/869\n",
      "Correct Answer: in the southern region, Response: Southern region, Eval: True. 850/869\n",
      "Correct Answer: NCAA Division I, Response: NCAA Division I, Eval: True. 851/869\n",
      "Correct Answer: over 1,200, Response: 1,200, Eval: False. 852/869\n",
      "Correct Answer: twice, Response: Twice a year., Eval: True. 853/869\n",
      "Correct Answer: 1869, Response: 1869, Eval: True. 854/869\n",
      "Correct Answer: Psychological, Response: Psychological anthropology., Eval: True. 855/869\n",
      "Correct Answer: 56, Response: 56, Eval: True. 856/869\n",
      "Correct Answer: about 325, Response: 325, Eval: True. 857/869\n",
      "Correct Answer: 51 °C, Response: 51 °C, Eval: True. 858/869\n",
      "Correct Answer: John B. Gordon, Response: General John B. Gordon, Eval: True. 859/869\n",
      "Correct Answer: Medical College of Virginia, Response: MCV Campus, Eval: True. 860/869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: Public Broadcasting, Response: Public Broadcasting Service (PBS), Eval: True. 861/869\n",
      "Correct Answer: growth, Response: Growth medium., Eval: True. 862/869\n",
      "Correct Answer: poaching, Response: Poaching., Eval: True. 863/869\n",
      "Correct Answer: pests, Response: pests, Eval: True. 864/869\n",
      "Correct Answer: 5,200,000 acres, Response: National Wildlife Refuge System lands., Eval: True. 865/869\n",
      "Correct Answer: Migratory Bird Hunting Stamp Act, Response: Migratory Bird Hunting Stamp Act, Eval: True. 866/869\n",
      "Correct Answer: Bagmati, Response: Bagmati, Eval: True. 867/869\n",
      "Correct Answer: northwestern, Response: Northwestern part, Eval: True. 868/869\n",
      "Correct Answer: National Academy of Medical Sciences, Response: National Academy of Medical Sciences, Eval: True. 869/869\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=sub_qa output=responses,evaluations\n",
    "responses = []\n",
    "evaluations = []\n",
    "ctr = 0\n",
    "for ent in sub_qa:\n",
    "    para_id, question, true_answer = ent\n",
    "    try:\n",
    "        true_answer, response, evaluation = do_work(client, question, true_answer, all_paragraphs[para_id])\n",
    "    except openai.RateLimitError:\n",
    "        time.sleep(10)\n",
    "        true_answer, response, evaluation = do_work(client, question, true_answer, all_paragraphs[para_id])\n",
    "    ctr += 1\n",
    "    print(f\"Correct Answer: {true_answer}, Response: {response}, Eval: {evaluation}. {ctr}/{len(sub_qa)}\")\n",
    "    responses.append(response)\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f08e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%xmemo input=sub_qa,responses,evaluations output=df\n",
    "df = pd.DataFrame()\n",
    "df['context'] = [all_paragraphs[i[0]] for i in sub_qa]\n",
    "df['question'] = [i[1] for i in sub_qa]\n",
    "df['answer'] = [i[2] for i in sub_qa]\n",
    "df['response'] = responses\n",
    "df['evaluations'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa3c1e4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "      <th>evaluations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>Late 1990s</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The remaining band members recorded \"Independe...</td>\n",
       "      <td>How many weeks did their single \"Independent W...</td>\n",
       "      <td>eleven</td>\n",
       "      <td>11 weeks</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At the 52nd Annual Grammy Awards, Beyoncé rece...</td>\n",
       "      <td>How many awards was Beyonce nominated for at t...</td>\n",
       "      <td>ten</td>\n",
       "      <td>Ten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At the 57th Annual Grammy Awards in February 2...</td>\n",
       "      <td>Which artist beat Beyonce out for Album of the...</td>\n",
       "      <td>Beck</td>\n",
       "      <td>Beck</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forbes magazine began reporting on Beyoncé's e...</td>\n",
       "      <td>In 2012 who placed Beyonce at 16 in the Celebr...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Each year, nearly $200 million in hunters' fed...</td>\n",
       "      <td>What does land has Federal Duck Stamp money he...</td>\n",
       "      <td>5,200,000 acres</td>\n",
       "      <td>National Wildlife Refuge System lands.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>On 16 March 1934, President Franklin D. Roosev...</td>\n",
       "      <td>What act was signed in 1934?</td>\n",
       "      <td>Migratory Bird Hunting Stamp Act</td>\n",
       "      <td>Migratory Bird Hunting Stamp Act</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Kathmandu is located in the northwestern part ...</td>\n",
       "      <td>What river is south of Kathmandu?</td>\n",
       "      <td>Bagmati</td>\n",
       "      <td>Bagmati</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>Swayambhu is a Buddhist stupa atop a hillock a...</td>\n",
       "      <td>In what part of Kathmandu is Swayambhu located?</td>\n",
       "      <td>northwestern</td>\n",
       "      <td>Northwestern part</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>Institute of Medicine, the central college of ...</td>\n",
       "      <td>What institution of tertiary education is know...</td>\n",
       "      <td>National Academy of Medical Sciences</td>\n",
       "      <td>National Academy of Medical Sciences</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context   \n",
       "0    Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \\\n",
       "1    The remaining band members recorded \"Independe...   \n",
       "2    At the 52nd Annual Grammy Awards, Beyoncé rece...   \n",
       "3    At the 57th Annual Grammy Awards in February 2...   \n",
       "4    Forbes magazine began reporting on Beyoncé's e...   \n",
       "..                                                 ...   \n",
       "864  Each year, nearly $200 million in hunters' fed...   \n",
       "865  On 16 March 1934, President Franklin D. Roosev...   \n",
       "866  Kathmandu is located in the northwestern part ...   \n",
       "867  Swayambhu is a Buddhist stupa atop a hillock a...   \n",
       "868  Institute of Medicine, the central college of ...   \n",
       "\n",
       "                                              question   \n",
       "0             When did Beyonce start becoming popular?  \\\n",
       "1    How many weeks did their single \"Independent W...   \n",
       "2    How many awards was Beyonce nominated for at t...   \n",
       "3    Which artist beat Beyonce out for Album of the...   \n",
       "4    In 2012 who placed Beyonce at 16 in the Celebr...   \n",
       "..                                                 ...   \n",
       "864  What does land has Federal Duck Stamp money he...   \n",
       "865                       What act was signed in 1934?   \n",
       "866                  What river is south of Kathmandu?   \n",
       "867    In what part of Kathmandu is Swayambhu located?   \n",
       "868  What institution of tertiary education is know...   \n",
       "\n",
       "                                   answer   \n",
       "0                       in the late 1990s  \\\n",
       "1                                  eleven   \n",
       "2                                     ten   \n",
       "3                                    Beck   \n",
       "4                                  Forbes   \n",
       "..                                    ...   \n",
       "864                       5,200,000 acres   \n",
       "865      Migratory Bird Hunting Stamp Act   \n",
       "866                               Bagmati   \n",
       "867                          northwestern   \n",
       "868  National Academy of Medical Sciences   \n",
       "\n",
       "                                   response  evaluations  \n",
       "0                                Late 1990s         True  \n",
       "1                                  11 weeks         True  \n",
       "2                                       Ten         True  \n",
       "3                                      Beck         True  \n",
       "4                                    Forbes         True  \n",
       "..                                      ...          ...  \n",
       "864  National Wildlife Refuge System lands.         True  \n",
       "865        Migratory Bird Hunting Stamp Act         True  \n",
       "866                                 Bagmati         True  \n",
       "867                       Northwestern part         True  \n",
       "868    National Academy of Medical Sciences         True  \n",
       "\n",
       "[869 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb69d44",
   "metadata": {},
   "source": [
    "# OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c02a61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "def embed_entry(text):\n",
    "    global ctr\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.embeddings.create(input=text, model='text-embedding-ada-002', encoding_format='float')\n",
    "            ctr += 1\n",
    "            if ctr % 100 == 0:\n",
    "                print(ctr)\n",
    "        except openai.RateLimitError as e:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def generate_openai_embeddings(client, textlist):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        ret = list(executor.map(embed_entry, textlist))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71d9ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 6f9fdedb188cbc14a74921efe44e51202f2878fd483cc3648fa869888fd39322.pickle\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=embed_entry,all_paragraphs output=par_embeddings\n",
    "par_embeddings = generate_openai_embeddings(client, all_paragraphs)\n",
    "par_embeddings = np.array(par_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f93a90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n",
      "85800\n",
      "85900\n",
      "86000\n",
      "86100\n",
      "86200\n",
      "86300\n",
      "86400\n",
      "86500\n",
      "86600\n",
      "86700\n",
      "86800\n",
      "86900\n",
      "87000\n",
      "87100\n",
      "87200\n",
      "87300\n",
      "87400\n",
      "87500\n"
     ]
    }
   ],
   "source": [
    "%%xmemo input=embed_entry,all_qa output=qn_embeddings\n",
    "qn_embeddings = generate_openai_embeddings(client, [q[1] for q in all_qa])\n",
    "qn_embeddings = np.array(qn_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee70c3",
   "metadata": {},
   "source": [
    "# Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8ae062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbb71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_embeddings_arr = np.load('data/openai_paragraphs_embeddings.npy')\n",
    "qn_embeddings_arr = np.load('data/openai_qn_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faac1647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;ball_tree&#x27;, n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;ball_tree&#x27;, n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', n_neighbors=10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nbrs.fit(par_embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = nbrs.kneighbors(qn_embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7f3081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.53342373, 0.54183945, 0.5480534 , ..., 0.55267631, 0.55477935,\n",
       "         0.55652812],\n",
       "        [0.51625546, 0.56791918, 0.57577417, ..., 0.60289394, 0.60317292,\n",
       "         0.60357273],\n",
       "        [0.47841394, 0.49049756, 0.51447069, ..., 0.54367424, 0.5441335 ,\n",
       "         0.54597068],\n",
       "        ...,\n",
       "        [0.57681319, 0.59904399, 0.59999977, ..., 0.6253042 , 0.62548659,\n",
       "         0.62892828],\n",
       "        [0.49743956, 0.57099157, 0.5722643 , ..., 0.59779   , 0.60029111,\n",
       "         0.60105112],\n",
       "        [0.6205644 , 0.67376102, 0.68988245, ..., 0.70489665, 0.70647834,\n",
       "         0.70827299]]),\n",
       " array([[   23,    45,    26, ...,    50,     0,    14],\n",
       "        [    4,     5,     3, ...,    64,    42,    20],\n",
       "        [    1,    11,     8, ...,     7,    23,    36],\n",
       "        ...,\n",
       "        [18979, 18929, 18925, ..., 18966, 18972, 18934],\n",
       "        [18979, 18932, 18940, ..., 18978, 18972, 18941],\n",
       "        [18979, 18255,  3350, ...,   126, 13786, 11440]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/openai_knn_qn_to_paragraph.npy', knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924ae964",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nearest_paragraphs = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e9c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_paragraphs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa7e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([i[0] for i in all_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f1bcc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at = []\n",
    "p_at.append((nearest_paragraphs[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (nearest_paragraphs[:,i] == truth).sum()\n",
    "    p_at.append(p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e7395aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54566, 63738, 67928, 70421, 72127, 73358, 74400, 75232, 75857, 76395]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4c925",
   "metadata": {},
   "source": [
    "# Simpler Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20857323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45b2f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ylow/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11003763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "import concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9eb5ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    words = nltk.tokenize.word_tokenize(sentence)\n",
    "    return [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a15f1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenize(par) for par in all_paragraphs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cdd157c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bm25_helpers' from '/Users/ylow/xetrepos/RagIRBench/bm25_helpers.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import bm25_helpers\n",
    "importlib.reload(bm25_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8562ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "10852\n",
      "16278\n",
      "21705\n",
      "27131\n",
      "32557\n",
      "37984\n",
      "43410\n",
      "48836\n",
      "54263\n",
      "59689\n",
      "65115\n",
      "70542\n",
      "75968\n",
      "81394\n",
      "86821\n"
     ]
    }
   ],
   "source": [
    "bm25_nearest_paragraphs = []\n",
    "futures = []\n",
    "ctr = 0\n",
    "nworkers = 8\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for s in range(nworkers):\n",
    "        start = (s * len(all_qa)) // nworkers\n",
    "        end = ((s + 1) * len(all_qa)) // nworkers\n",
    "        if s == nworkers - 1:\n",
    "            end = len(all_qa)\n",
    "        futures.append(executor.submit(bm25_helpers.get_bm25_top_10_batch, bm25, [qa[1] for qa in all_qa[start:end]]))\n",
    "\n",
    "    for future in futures:\n",
    "        bm25_nearest_paragraphs.extend(future.result())\n",
    "        print(len(bm25_nearest_paragraphs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b01b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.array(bm25_nearest_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a25a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/bm25_knn_qn_to_paragraph.npy', bm25_nearest_paragraphs_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f61e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.load('data/bm25_knn_qn_to_paragraph.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef825211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6025, 15710, 10808, ..., 11323,  5977, 17616],\n",
       "       [ 9402,  9371, 10274, ..., 11248, 17503,  1679],\n",
       "       [   36,     8,     1, ...,     7,  3685,    21],\n",
       "       ...,\n",
       "       [18938, 18935, 18936, ..., 18945, 18933, 18963],\n",
       "       [18939, 18945, 18940, ..., 18976, 18932, 18934],\n",
       "       [14150, 14946,  1576, ..., 13842,  9371, 10836]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_nearest_paragraphs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baebadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_p_at = []\n",
    "bm25_p_at.append((bm25_nearest_paragraphs_arr[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (bm25_nearest_paragraphs_arr[:,i] == truth).sum()\n",
    "    bm25_p_at.append(bm25_p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7d024d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42140, 49077, 52628, 54824, 56524, 57858, 58863, 59752, 60493, 61126]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_p_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64e499",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb8d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_augment_cache = {}\n",
    "try:\n",
    "    query_augment_cache = pickle.load(open('data/query_augment_cache.pickle','rb'))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17b516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_augment(client, question):\n",
    "    global query_augment_cache\n",
    "    key = question\n",
    "    if key in query_augment_cache:\n",
    "        return query_augment_cache[key]\n",
    "\n",
    "    system_prompt = \"You are to perform query augmentation for a question answering system. For the provided question, return a comma separated list of up to 5 new words which are not part of the question that can be used to improve document retrieval. These words will be added to the words already in the question and will be used to search a collection of documents to find a document which may contain the answer.\"\n",
    "    user_prompt = f\"Question: {question}\"\n",
    "    message = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                }]\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                    messages=message,\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=0.0)\n",
    "        except openai.RateLimitError as e:\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    # split by , \n",
    "    r = response.choices[0].message.content\n",
    "    response = response.choices[0].message.content.split(',')\n",
    "    # split by space\n",
    "    response = [r.strip().split(' ') for r in response]\n",
    "    # flatten\n",
    "    response = [word for i in response for word in i]\n",
    "    response = list(set(response))\n",
    "    query_augment_cache[key] = response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6be50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(max_retries=5,timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17b4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent\n",
    "question_augmentation = []\n",
    "def augment_lambda(ent):\n",
    "    ret = query_augment(client, ent[1])\n",
    "    print(ret)\n",
    "    return ret\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    question_augmentation = list(executor.map(augment_lambda, all_qa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed0829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/query_augment_cache.pickle','wb')\n",
    "pickle.dump(query_augment_cache, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68c8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/qa_question_augmentation.pickle','wb')\n",
    "pickle.dump(question_augmentation, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7a54b",
   "metadata": {},
   "source": [
    "# Using Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49eb17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "tokenized_corpus = [bm25_helpers.tokenize(par) for par in all_paragraphs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a7a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10852\n",
      "21705\n",
      "32557\n",
      "43410\n",
      "54263\n",
      "65115\n",
      "75968\n",
      "86821\n"
     ]
    }
   ],
   "source": [
    "bm25_nearest_paragraphs = []\n",
    "futures = []\n",
    "ctr = 0\n",
    "nworkers = 8\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for s in range(nworkers):\n",
    "        start = (s * len(all_qa)) // nworkers\n",
    "        end = ((s + 1) * len(all_qa)) // nworkers\n",
    "        if s == nworkers - 1:\n",
    "            end = len(all_qa)\n",
    "        futures.append(executor.submit(bm25_helpers.get_bm25_top_10_batch_augmented, bm25, \n",
    "                                       [qa[1] for qa in all_qa[start:end]],\n",
    "                                       question_augmentation[start:end]))\n",
    "\n",
    "    for future in futures:\n",
    "        bm25_nearest_paragraphs.extend(future.result())\n",
    "        print(len(bm25_nearest_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95d74c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nearest_paragraphs_arr = np.array(bm25_nearest_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a516e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/bm25_knn_qn_to_paragraph_augmented.npy', bm25_nearest_paragraphs_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cf2f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([i[0] for i in all_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3a677d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_p_at = []\n",
    "bm25_p_at.append((bm25_nearest_paragraphs_arr[:,0] == truth).sum())\n",
    "for i in range(1,10):\n",
    "    p = (bm25_nearest_paragraphs_arr[:,i] == truth).sum()\n",
    "    bm25_p_at.append(bm25_p_at[i-1] + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8599c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50821, 59938, 64156, 66789, 68640, 70104, 71252, 72189, 73027, 73692]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_p_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135ed91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
