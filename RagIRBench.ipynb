{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cea69652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import openai\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e439c9",
   "metadata": {},
   "source": [
    "# Process the Squad JSON to extract what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4290fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json('data/squad2.json')['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "696cbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = [paragraph['context'] for ent in d for paragraph in ent['paragraphs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9ace2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. list comprehension is still possible but it gets a little obnoxious.\n",
    "all_qa = []\n",
    "paragraph_id = 0\n",
    "for ent in d:\n",
    "    for paragraph in ent['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0 and qa['is_impossible'] == False:\n",
    "                all_qa.append((paragraph_id, qa['question'], qa['answers'][0]['text']))\n",
    "        paragraph_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a08914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "de9bf620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19035"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bef78fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "88193838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'When did Beyonce start becoming popular?', 'in the late 1990s'),\n",
       " (0,\n",
       "  'What areas did Beyonce compete in when she was growing up?',\n",
       "  'singing and dancing'),\n",
       " (0,\n",
       "  \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       "  '2003'),\n",
       " (0, 'In what city and state did Beyonce  grow up? ', 'Houston, Texas'),\n",
       " (0, 'In which decade did Beyonce become famous?', 'late 1990s')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_qa[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3802125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/paragraphs_and_qa.pickle','wb')\n",
    "pickle.dump(all_paragraphs, f)\n",
    "pickle.dump(all_qa, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e71a64",
   "metadata": {},
   "source": [
    "# Baseline OpenAI QA. Knowing exactly the paragraphs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ea43f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/paragraphs_and_qa.pickle','rb')\n",
    "all_paragraphs = pickle.load(f)\n",
    "all_qa = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc4fb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(max_retries=5,timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42487e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "733e2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answerer(client, question, context):\n",
    "    global qa_cache\n",
    "    key = context + question\n",
    "    if key in qa_cache:\n",
    "        return qa_cache[key]\n",
    "    system_prompt = \"You are an assistant for question-answering tasks. Use the provided pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Provide just the answer in as few words as possible. Do not use complete sentences.\"\n",
    "    user_prompt = f\"Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    qa_cache[key] = response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ecfa30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "When did Beyonce start becoming popular?\n",
      "in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "para_id, question, answer = all_qa[0]\n",
    "print(all_paragraphs[para_id])\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "64085f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late 1990s\n"
     ]
    }
   ],
   "source": [
    "response = question_answerer(client, question, all_paragraphs[para_id])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdce6d",
   "metadata": {},
   "source": [
    "## Coming with a simple comparison evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a932d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def is_same(client, question, a1, a2):\n",
    "    system_prompt = \"You are an assistant for scoring answers. Two answers to a hypothetical question are provided. Say 'Yes' if both answers have the same meaning, and 'No' otherwise.\"\n",
    "    user_prompt = f\"Question: {question} \\Answer 1: {a1} \\nAnswer 2: {a2}\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content == 'Yes'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5580d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the late 1990s'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4c163de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, qn, answer, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d396228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same(client, qn, answer, \"Late 1980s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a215b",
   "metadata": {},
   "source": [
    "# Evaluate Everything\n",
    "80k questions at about 1k tokens per question == ~ 0.0010 * 80000 or about $80. That is a bit pricy for a quick test. We will subsample to ~ 10% questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "393430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sub_qa = all_qa[0:len(all_qa):2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "481f357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ce912ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_work(client, question, true_answer, context):\n",
    "    response = question_answerer(client, question, context)\n",
    "    evaluation = is_same(client, question, true_answer, response)\n",
    "    return true_answer, response, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ccc8e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: in the late 1990s, Response: Late 1990s, Eval: True\n",
      "Correct Answer: Peter Oppenheimer, Response: Peter Oppenheimer, Eval: True\n",
      "Correct Answer: gender and class, Response: gender and class, Eval: True\n",
      "Correct Answer: guitar, Response: guitar, Eval: True\n",
      "Correct Answer: CFS Leitrim in Ottawa, Response: CFS Leitrim in Ottawa., Eval: True\n",
      "Correct Answer: The Campus, Response: The Campus, Eval: True\n",
      "Correct Answer: Project Mercury, Response: Project Mercury, Eval: True\n",
      "Correct Answer: incandescent, Response: Incandescent lighting, Eval: True\n",
      "Correct Answer: early Christian liturgical music,, Response: Early Christian liturgical music., Eval: True\n",
      "Correct Answer: September 21, 19 BC, Response: September 21, 19 BC., Eval: True\n",
      "Correct Answer: ambiguity, Response: Ambiguity., Eval: True\n",
      "Correct Answer: 1931, Response: 1931, Eval: True\n",
      "Correct Answer: one of the top two, Response: Top two., Eval: True\n",
      "Correct Answer: Electromagnetic Aircraft Launch System (EMALS), Response: Electromagnetic Aircraft Launch System (EMALS), Eval: True\n",
      "Correct Answer: 2005, Response: 2005, Eval: True\n",
      "Correct Answer: a French artillery battery, Response: French artillery battery, Eval: True\n",
      "Correct Answer: 1,200, Response: 1,200, Eval: True\n",
      "Correct Answer: combination therapy, Response: Combination therapy, Eval: True\n",
      "Correct Answer: Antigonus II Mattathias, Response: Antigonus II Mattathias, Eval: True\n",
      "Correct Answer: Nakdong River, Response: Nakdong River, Eval: True\n",
      "Correct Answer: 2013, Response: 2013, Eval: True\n",
      "Correct Answer: Richard Dawkins, Response: Richard Dawkins, Eval: True\n",
      "Correct Answer: senior sub-editor, Response: Senior sub-editor, Eval: True\n",
      "Correct Answer: 1955, Response: 1955, Eval: True\n",
      "Correct Answer: single character, Response: Characters, Eval: False\n",
      "Correct Answer: an effort towards inclusion rather than a discriminatory practice, Response: Effort towards inclusion., Eval: True\n",
      "Correct Answer: \"big book\" on Natural Selection,, Response: \"big book\" on Natural Selection, Eval: True\n",
      "Correct Answer: mourning for the death of George III, Response: Don't know., Eval: False\n",
      "Correct Answer: Tibetan or Sanskrit, Response: Tibetan, Sanskrit, Eval: True\n",
      "Correct Answer: hotel rooms and office space, Response: Hotel rooms and office space., Eval: True\n",
      "Correct Answer: deeply unpopular, Response: Unpopular., Eval: True\n",
      "Correct Answer: Typhon, Response: Typhon, Eval: True\n",
      "Correct Answer: available energy, Response: Available energy, Eval: True\n",
      "Correct Answer: 38.55%, Response: 38.55%, Eval: True\n",
      "Correct Answer: Algiers, Response: Algiers, Eval: True\n",
      "Correct Answer: the animals on the endangered species list still were not getting enough protection, thus further threatening their extinction, Response: Expanded the endangered species program., Eval: False\n",
      "Correct Answer: paraphyletic, Response: primitive species, Eval: False\n",
      "Correct Answer: the three most important centres of Greek learning were situated in Chios, Smyrna and Aivali, all three major centres of Greek commerce, Response: Chios, Smyrna, Aivali, Eval: True\n",
      "Correct Answer: Hadrian's Wall, Response: Hadrian's Wall, Eval: True\n",
      "Correct Answer: 11 years, Response: 11 years, Eval: True\n",
      "Correct Answer: carmine, Response: carmine, Eval: True\n",
      "Correct Answer: Câmara Legislativa, Response: Câmara Legislativa, Eval: True\n",
      "Correct Answer: Grupo RAR's Vitacress, Sovena, Lactogal, Vale da Rosa, Companhia das Lezírias and Valouro, Response: Companies like Grupo RAR's Vitacress, Sovena, Lactogal, Vale da Rosa, Companhia das Lezírias, and Valouro., Eval: True\n",
      "Correct Answer: Public Broadcasting, Response: Public Broadcasting Service (PBS), Eval: True\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "evaluations = []\n",
    "futures = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    for ent in sub_qa:\n",
    "        para_id, question, true_answer = ent\n",
    "        future = executor.submit(do_work, client, question, true_answer, all_paragraphs[para_id])\n",
    "        futures.append(future)\n",
    "    for fut in futures:\n",
    "        true_answer, response, evaluation = fut.result()\n",
    "        print(f\"Correct Answer: {true_answer}, Response: {response}, Eval: {evaluation}\")\n",
    "        responses.append(response)\n",
    "        evaluations.append(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
